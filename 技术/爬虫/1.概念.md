# 一、概述

## 1.原理
网络爬虫又称为“网络蜘蛛”，是一个用来实现自动采集网络数据的程序。
如果将互联网比作一张蜘蛛网，互联网上的一个个网页比作蜘蛛网上的一个个节点，那么网页与网页之间的链接关系可以比作节点间的连线，而网络爬虫就可以比作在网上爬来爬去的蜘蛛。蜘蛛爬到一个节点相当于访问了该网页，提取了信息，然后顺着节点间的连线继续爬行到下一个节点，这样周而复始，蜘蛛就可以爬遍整个网络的所有节点，抓取数据。
## 2.步骤

```
1）预先设定一个或若干个初始网页的URL，将初始URL加入待爬取URL列表中。
2）从待爬取列表中逐个读取URL，并将URL加入已爬取URL列表中，然后下载网页。
3）解析已下载的网页，提取所需的数据和新的URL，并存储提取的数据。
4）将新的URL与已爬取的URL列表进行比对，检查该网页是否已爬取，如果网页没有被爬取，则将新的URL放入待爬取URL列表的末尾，等待读取。
5）如此往复，直到待爬取URL列表为空或者满足设定的终止条件，最终达到遍历网页的目的。
```
## 3.url

```
统一资源定位符（Uniform Resource Locator,URL）是对可以从互联网上得到的资源位置和访问方法的一种简洁表示，是互联网上标准资源的地址
```
## 4.分类
### 1).通用网络爬虫又称全网爬虫

```
是根据网络爬虫的基本原理实现的，它所爬取的目标会从初始设定的URL扩充到全网。通用网络爬虫主要应用于门户网站、搜索引擎和大型网络服务提供商的数据采集。
```
### 2).聚焦网络爬虫

```
爬取的目标是与预先定义好的主题相关的网页。与通用网络爬虫相比，聚焦网络爬虫只选择爬取与主题相关的网页，极大地节省了硬件和网络资源，它主要应用于对特定领域信息有需求的场景。聚焦网络爬虫在通用网络爬虫的基础上，需要对提取的新URL进行过滤处理，过滤掉与目标主题无关的网页，且根据一定的相关性搜索策略，确定待爬取URL列表的读取顺序。
```

### 3）增量式网络爬虫

```
爬取的目标是有更新的已下载网页和新产生的网页。爬虫程序监测网站数据更新的情况，然后在需要的时候只爬取发生更新或新产生的网页。这样，可有效减少数据下载量，及时更新已爬取的网页，但是增加了爬行算法的复杂度和实现难度。增量式网络爬虫主要应用于网页内容会时常更新的网站，或者不断有新网页出现的网站。
```

### 4).深层网络爬虫

```
爬取的目标是不能通过静态链接获取的，隐藏在搜索表单后的，只有用户提交一些关键词才能获得的网页，如用户注册后才可显示内容的网页。
```

# 二、网络爬虫工作流程

网络爬虫是一个自动化的程序，它的工作流程非常简单。爬虫程序首先发送请求，获取网页响应的内容，然后解析网页内容，最后将提取的数据存储到文件或数据库中。总结起来，其工作流程可以分为爬取网页、解析网页和存储数据三个步骤。

```
爬取网页----->解析网页----->存储数据
```

## 1.爬取网页

```
爬取网页后，接下来就是解析网页了。解析网页是用户根据网页结构，分析网页源代码，从中提取想要的数据。它可以使杂乱的数据变得条理清晰，以便用户后续处理和分析。
解析网页万能的方法是正则表达式，但是构造正则表达式比较复杂且容易出错，所以Python根据网页节点属性、CSS选择器及XPath语法提供了网页的解析库，如beautifulsoup4、lxml库等。使用这些库，用户可以高效快速地解析网页。
```

## 2.存储数据

```
解析网页提取数据后，一般要将提取到的数据保存起来以便后续使用。保存数据的方式有很多种，可以将其简单保存到JSON或CSV文件中，也可以保存到数据库中，如MySQL和MongoDB等。
```

# 三、网络爬虫协议

## 1.合法性

虽然互联网世界已经通过自己的规则建立了一定的道德规范（Robots协议），但法律部分还在建立和完善中。从目前的情况来看，**如果抓取的数据用于个人使用或科学研究，那么基本上是不违法的**；但如果数据用于其他用途，尤其是转载或商业用途，那么根据爬取网站数据的不同情况有不同的后果，严重的将会引起民事纠纷甚至触犯法律。因此，用户在爬取数据时应避免以下几个方面的问题。

```
1）侵犯著作权。
2）侵犯商业秘密。
3）侵犯个人隐私。
4）构成不正当竞争。
5）侵入计算机系统，构成刑事犯罪。
```

## 2.Robots协议

Robots协议（又称“爬虫协议”）的全称是“**网络爬虫排除标准**”（Robots exclusion protocol）。网站管理者可以通过它来表达是否希望爬虫程序自动获取网站信息的意愿。

管理者可以在网站根目录下放置一个robots.txt文件，并在文件中列出哪些链接不允许爬虫程序获取。当爬虫程序访问一个网站时，它会首先检查该网站根目录下是否存在robots.txt文件，如果存在，爬虫程序就会按照该文件中的内容来确定访问范围；如果不存在，爬虫程序就能够访问网站上所有没被保护的网页。

![image-20240709185757489](D:/%E6%96%87%E6%A1%A3/%E7%AC%94%E8%AE%B0/%E6%8A%80%E6%9C%AF/%E7%88%AC%E8%99%AB/image-20240709185757489.png)

```
User-agent: Googlebot-Image：表示这部分代码针对谷歌图片爬虫，禁止或允许谷歌图片爬虫爬取某些文件
User-agent:*：表示针对所有搜索引擎的爬虫程序。
Disallow: /appview/：表示禁止爬取网站根目录的appview文件夹下的文件。
Disallow: /*?guide*：表示禁止爬取网站中所有包含guide的网址。
Allow: /search-special：表示允许爬取网站根目录下所有以search-special开头的文件夹和文件。
```

![image-20240709190446659](D:/%E6%96%87%E6%A1%A3/%E7%AC%94%E8%AE%B0/%E6%8A%80%E6%9C%AF/%E7%88%AC%E8%99%AB/image-20240709190446659.png)

```
当然，如果要禁止爬虫程序爬取网站中的所有内容，可以用更简单的方法。
例如，淘宝网不允许百度的爬虫程序访问其网站下所有的目录，其robots.txt文件内容
```