# 一、获取数据
## 一.http基本原理
浏览器中输入一个URL，按回车键后便会在浏览器中显示网页内容。实际上，这个过程是浏览器向Web服务器发送了一个HTTP请求；Web服务器接收到这个请求后进行解析和处理，然后返回给浏览器对应的HTTP响应；浏览器再对HTTP响应进行解析，从而将网页呈现了出来。下面以使用Google Chrome浏览器打开百度的搜索页面为例来展示HTTP请求和响应的过程。
### 1.HTTP和HTTPS
#### 1).概念

`HTTP协议`（HyperText Transfer Protocol，超文本传输协议）：是一种发布和接收 HTML页面的方法。

`HTTPS`（Hypertext Transfer Protocol over Secure Socket Layer）简单讲是HTTP的安全版，在HTTP下加入SSL层。

`SSL`（Secure Sockets Layer 安全套接层）主要用于Web的安全传输协议，在传输层对网络连接进行加密，保障在Internet上数据传输的安全。

- `HTTP`的端口号为`80`，
- `HTTPS`的端口号为`443`

#### 2).HTTP的请求与响应

HTTP通信由两部分组成： **客户端请求消息** 与 **服务器响应消息**

![img](D:\文档\笔记\02_http_pro-1710807457387-1.jpg)

**浏览器发送HTTP请求的过程：**

1. 当用户在浏览器的地址栏中输入一个URL并按回车键之后，浏览器会向HTTP服务器发送HTTP请求。HTTP请求主要分为“Get”和“Post”两种方法。
2. 当我们在浏览器输入URL http://www.baidu.com 的时候，浏览器发送一个Request请求去获取 http://www.baidu.com 的html文件，服务器把Response文件对象发送回给浏览器。
3. 浏览器分析Response中的 HTML，发现其中引用了很多其他文件，比如Images文件，CSS文件，JS文件。 浏览器会自动再次发送Request去获取图片，CSS文件，或者JS文件。
4. 当所有的文件都下载成功后，网页会根据HTML语法结构，完整的显示出来了。

URL（Uniform / Universal Resource Locator的缩写）：统一资源定位符，是用于完整地描述Internet上网页和其他资源的地址的一种标识方法。

![img](D:\文档\笔记\01-httpstruct-1710807482993-4.jpg)

基本格式：`scheme://host[:port#]/path/…/[?query-string][#anchor]`

- scheme：协议(例如：http, https, ftp)
- host：服务器的IP地址或者域名
- port#：服务器的端口（如果是走协议默认端口，缺省端口80）
- path：访问资源的路径
- query-string：参数，发送给http服务器的数据
- anchor：锚（跳转到网页的指定锚点位置）

例如：

- [ftp://192.168.0.116:8080/index](ftp://192.168.0.116:8080/index)
- http://www.baidu.com
- http://item.jd.com/11936238.html#product-detail

### 2.客户端HTTP请求

URL只是标识资源的位置，而HTTP是用来提交和获取资源。客户端发送一个HTTP请求到服务器的请求消息，包括以下格式：

```
请求行`、`请求头部`、`空行`、`请求数据
```

四个部分组成，下图给出了请求报文的一般格式。

![img](D:\文档\笔记\01_request-1710807499144-7.png)

**一个典型的HTTP请求示例**

```
GET https://www.baidu.com/ HTTP/1.1
Host: www.baidu.com
Connection: keep-alive
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
Referer: http://www.baidu.com/
Accept-Encoding: gzip, deflate, sdch, br
Accept-Language: zh-CN,zh;q=0.8,en;q=0.6
Cookie: BAIDUID=04E4001F34EA74AD4601512DD3C41A7B:FG=1; BIDUPSID=04E4001F34EA74AD4601512DD3C41A7B; PSTM=1470329258; MCITY=-343%3A340%3A; BDUSS=nF0MVFiMTVLcUh-Q2MxQ0M3STZGQUZ4N2hBa1FFRkIzUDI3QlBCZjg5cFdOd1pZQVFBQUFBJCQAAAAAAAAAAAEAAADpLvgG0KGyvLrcyfrG-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFaq3ldWqt5XN; H_PS_PSSID=1447_18240_21105_21386_21454_21409_21554; BD_UPN=12314753; sug=3; sugstore=0; ORIGIN=0; bdime=0; H_PS_645EC=7e2ad3QHl181NSPbFbd7PRUCE1LlufzxrcFmwYin0E6b%2BW8bbTMKHZbDP0g; BDSVRTM=0
```

### 3.请求方法

```
GET https://www.baidu.com/ HTTP/1.1
```

根据HTTP标准，HTTP请求可以使用多种请求方法。

HTTP 0.9：只有基本的文本 GET 功能。

HTTP 1.0：完善的请求/响应模型，并将协议补充完整，定义了三种请求方法： GET, POST 和 HEAD方法。

HTTP 1.1：在 1.0 基础上进行更新，新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法。

HTTP 2.0（未普及）：请求/响应首部的定义基本没有改变，只是所有首部键必须全部小写，而且请求行要独立为 :method、:scheme、:host、:path这些键值对。

| 序号 | 方法    | 描述                                                         |
| ---- | ------- | ------------------------------------------------------------ |
| 1    | GET     | 请求指定的页面信息，并返回实体主体。                         |
| 2    | HEAD    | 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 |
| 3    | POST    | 向指定资源提交数据进行处理请求（例如提交表单或者上传文件），数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 |
| 4    | PUT     | 从客户端向服务器传送的数据取代指定的文档的内容。             |
| 5    | DELETE  | 请求服务器删除指定的页面。                                   |
| 6    | CONNECT | HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。     |
| 7    | OPTIONS | 允许客户端查看服务器的性能。                                 |
| 8    | TRACE   | 回显服务器收到的请求，主要用于测试或诊断。                   |

#### HTTP请求主要分为Get和Post两种方法

- GET是从服务器上获取数据，POST是向服务器传送数据
- GET请求参数显示，都显示在浏览器网址上，HTTP服务器根据该请求所包含URL中的参数来产生响应内容，即“Get”请求的参数是URL的一部分。 例如： `http://www.baidu.com/s?wd=Chinese`
- POST请求参数在请求体当中，消息长度没有限制而且以隐式的方式进行发送，通常用来向HTTP服务器提交量比较大的数据（比如请求中包含许多参数或者文件上传操作等），请求的参数包含在“Content-Type”消息头里，指明该消息体的媒体类型和编码，

**注意：避免使用Get方式提交表单，因为有可能会导致安全问题。 比如说在登陆表单中用Get方式，用户输入的用户名和密码将在地址栏中暴露无遗。**

### 4.常用的请求报头

```
					a.Host (主机和端口号)
Host：对应网址URL中的Web名称和端口号，用于指定被请求资源的Internet主机和端口号，通常属于URL的一部分。

					b.Connection (链接类型)
Connection：表示客户端与服务连接类型
    1. Client 发起一个包含 `Connection:keep-alive` 的请求，HTTP/1.1使用 `keep-alive` 为默认值。
    2. Server收到请求后：
        	a.如果 Server 支持 keep-alive，回复一个包含 Connection:keep-alive 的响应，不关闭连接；
        	b.如果 Server 不支持 keep-alive，回复一个包含 Connection:close 的响应，关闭连接。
    3. 如果client收到包含 `Connection:keep-alive` 的响应，向同一个连接发送下一个请求，直到一方主动关闭连接。
			keep-alive在很多情况下能够重用连接，减少资源消耗，缩短响应时间，比如当浏览器需要多个文件时(比如一个HTML文件和相关的图形文件)，不需要每次都去请求建立连接。

					c.Upgrade-Insecure-Requests (升级为HTTPS请求)
Upgrade-Insecure-Requests：升级不安全的请求，意思是会在加载 http 资源时自动替换成 https 请求，让浏览器不再显示https页面中的http请求警报。
HTTPS 是以安全为目标的 HTTP 通道，所以在 HTTPS 承载的页面上不允许出现 HTTP 请求，一旦出现就是提示或报错。

					d.User-Agent (浏览器名称)
User-Agent：是客户浏览器的名称，以后会详细讲。

					e.Accept (传输文件类型)
Accept：指浏览器或其他客户端可以接受的MIME（Multipurpose Internet Mail Extensions（多用途互联网邮件扩展））文件类型，服务器可以根据它判断并返回适当的文件格式。
举例：
    Accept: */*：表示什么都可以接收。
    Accept：image/gif：表明客户端希望接受GIF图像格式的资源；
    Accept：text/html：表明客户端希望接受html文本。
    Accept: text/html, application/xhtml+xml;q=0.9, image/*;q=0.8：表示浏览器支持的 MIME 类型分别是 html文本、xhtml和xml文档、所有的图像格式资源。
		q是权重系数，范围 0 =< q <= 1，q 值越大，请求越倾向于获得其“;”之前的类型表示的内容。若没有指定q值，则默认为1，按从左到右排序顺序；若被赋值为0，则用于表示浏览器不接受此内容类型。\***
		Text：用于标准化地表示的文本信息，文本消息可以是多种字符集和或者多种格式的；Application：用于传输应用程序数据或者二进制数据。[详细请点击](http://blog.sina.com.cn/s/blog_866e403f010179f1.html)\***

					f.Referer (页面跳转处)
Referer：表明产生请求的网页来自于哪个URL，用户是从该 Referer页面访问到当前请求的页面。这个属性可以用来跟踪Web请求来自哪个页面，是从什么网站来的等。
有时候遇到下载某网站图片，需要对应的referer，否则无法下载图片，那是因为人家做了防盗链，原理就是根据referer去判断是否是本网站的地址，如果不是，则拒绝，如果是，就可以下载；

					g.Accept-Encoding（文件编解码格式）
Accept-Encoding：指出浏览器可以接受的编码方式。编码方式不同于文件格式，它是为了压缩文件并加速文件传递速度。浏览器在接收到Web响应之后先解码，然后再检查文件格式，许多情形下这可以减少大量的下载时间。
		举例：Accept-Encoding:gzip;q=1.0, identity; q=0.5, *;q=0
如果有多个Encoding同时匹配, 按照q值顺序排列，本例中按顺序支持 gzip, identity压缩编码，支持gzip的浏览器会返回经过gzip编码的HTML页面。 **如果请求消息中没有设置这个域服务器假定客户端对各种内容编码都可以接受。**

					h.Accept-Language（语言种类）
Accept-Langeuage：指出浏览器可以接受的语言种类，如en或en-us指英语，zh或者zh-cn指中文，当服务器能够提供一种以上的语言版本时要用到。

					i.Accept-Charset（字符编码）
Accept-Charset：指出浏览器可以接受的字符编码。
	举例：Accept-Charset:iso-8859-1,gb2312,utf-8
        ISO8859-1：通常叫做Latin-1。Latin-1包括了书写所有西方欧洲语言不可缺少的附加字符，英文浏览器的默认值是ISO-8859-1.
        gb2312：标准简体中文字符集;
        utf-8：UNICODE 的一种变长字符编码，可以解决多种语言文本显示问题，从而实现应用国际化和本地化。
			如果在请求消息中没有设置这个域，缺省是任何字符集都可以接受。

					j.Cookie （Cookie）
Cookie：浏览器用这个属性向服务器发送Cookie。Cookie是在浏览器中寄存的小型数据体，它可以记载和服务器相关的用户信息，也可以用来实现会话功能，以后会详细讲。

					k.Content-Type (POST数据类型)
Content-Type：POST请求里用来表示的内容类型。
	举例：Content-Type = Text/XML; charset=gb2312：
	指明该请求的消息体中包含的是纯文本的XML类型的数据，字符编码采用“gb2312”。
```

### 5.服务端HTTP响应

HTTP响应也由四个部分组成，分别是： `状态行`、`消息报头`、`空行`、`响应正文`

![img](D:\文档\笔记\01_response-1710807522597-10.jpg)

```
HTTP/1.1 200 OK
Server: Tengine
Connection: keep-alive
Date: Wed, 30 Nov 2016 07:58:21 GMT
Cache-Control: no-cache
Content-Type: text/html;charset=UTF-8
Keep-Alive: timeout=20
Vary: Accept-Encoding
Pragma: no-cache
X-NWS-LOG-UUID: bd27210a-24e5-4740-8f6c-25dbafa9c395
Content-Length: 180945

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" ....
```

### 6.常用的响应报头(了解)

理论上所有的响应头信息都应该是回应请求头的。但是服务端为了效率，安全，还有其他方面的考虑，会添加相对应的响应头信息，从上图可以看到：

```
### 1. Cache-Control：must-revalidate, no-cache, private。

这个值告诉客户端，服务端不希望客户端缓存资源，在下次请求资源时，必须要从新请求服务器，不能从缓存副本中获取资源。

- Cache-Control是响应头中很重要的信息，当客户端请求头中包含Cache-Control:max-age=0请求，明确表示不会缓存服务器资源时,Cache-Control作为作为回应信息，通常会返回no-cache，意思就是说，"那就不缓存呗"。
- 当客户端在请求头中没有包含Cache-Control时，服务端往往会定,不同的资源不同的缓存策略，比如说oschina在缓存图片资源的策略就是Cache-Control：max-age=86400,这个意思是，从当前时间开始，在86400秒的时间内，客户端可以直接从缓存副本中读取资源，而不需要向服务器请求。

### 2. Connection：keep-alive

这个字段作为回应客户端的Connection：keep-alive，告诉客户端服务器的tcp连接也是一个长连接，客户端可以继续使用这个tcp连接发送http请求。

### 3. Content-Encoding:gzip

告诉客户端，服务端发送的资源是采用gzip编码的，客户端看到这个信息后，应该采用gzip对资源进行解码。

### 4. Content-Type：text/html;charset=UTF-8

告诉客户端，资源文件的类型，还有字符编码，客户端通过utf-8对资源进行解码，然后对资源进行html解析。通常我们会看到有些网站是乱码的，往往就是服务器端没有返回正确的编码。

### 5. Date：Sun, 21 Sep 2016 06:18:21 GMT

这个是服务端发送资源时的服务器时间，GMT是格林尼治所在地的标准时间。http协议中发送的时间都是GMT的，这主要是解决在互联网上，不同时区在相互请求资源的时候，时间混乱问题。

### 6. Expires:Sun, 1 Jan 2000 01:00:00 GMT

这个响应头也是跟缓存有关的，告诉客户端在这个时间前，可以直接访问缓存副本，很显然这个值会存在问题，因为客户端和服务器的时间不一定会都是相同的，如果时间不同就会导致问题。所以这个响应头是没有Cache-Control：max-age=*这个响应头准确的，因为max-age=date中的date是个相对时间，不仅更好理解，也更准确。

### 7. Pragma:no-cache

这个含义与Cache-Control等同。

### 8.Server：Tengine/1.4.6

这个是服务器和相对应的版本，只是告诉客户端服务器的信息。

### 9. Transfer-Encoding：chunked

这个响应头告诉客户端，服务器发送的资源的方式是分块发送的。一般分块发送的资源都是服务器动态生成的，在发送时还不知道发送资源的大小，所以采用分块发送，每一块都是独立的，独立的块都能标示自己的长度，最后一块是0长度的，当客户端读到这个0长度的块时，就可以确定资源已经传输完了。

### 10. Vary: Accept-Encoding

告诉缓存服务器，缓存压缩文件和非压缩文件两个版本，现在这个字段用处并不大，因为现在的浏览器都是支持压缩的。
```

### 7.Cookie 和 Session：

服务器和客户端的交互仅限于请求/响应过程，结束之后便断开，在下一次请求时，服务器会认为新的客户端。

为了维护他们之间的链接，让服务器知道这是前一个用户发送的请求，必须在一个地方保存客户端的信息。

**Cookie**：通过在 客户端 记录的信息确定用户的身份。

**Session**：通过在 服务器端 记录的信息确定用户的身份。

### 8.响应状态码

响应状态代码有三位数字组成，第一个数字定义了响应的类别，且有五种可能取值。

#### 常见状态码：

- `100~199`：表示服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程。
- `200~299`：表示服务器成功接收请求并已完成整个处理过程。常用200（OK 请求成功）。
- `300~399`：为完成请求，客户需进一步细化请求。例如：请求的资源已经移动一个新地址、常用302（所请求的页面已经临时转移至新的url）、307和304（使用缓存资源）。
- `400~499`：客户端的请求有错误，常用404（服务器无法找到被请求的页面）、403（服务器拒绝访问，权限不够）。
- `500~599`：服务器端出现错误，常用500（请求未完成。服务器遇到不可预知的情况）。

### 9.HTTP响应状态码参考：

```python
1xx:信息

100 Continue
服务器仅接收到部分请求，但是一旦服务器并没有拒绝该请求，客户端应该继续发送其余的请求。
101 Switching Protocols
服务器转换协议：服务器将遵从客户的请求转换到另外一种协议。



2xx:成功

200 OK
请求成功（其后是对GET和POST请求的应答文档）
201 Created
请求被创建完成，同时新的资源被创建。
202 Accepted
供处理的请求已被接受，但是处理未完成。
203 Non-authoritative Information
文档已经正常地返回，但一些应答头可能不正确，因为使用的是文档的拷贝。
204 No Content
没有新文档。浏览器应该继续显示原来的文档。如果用户定期地刷新页面，而Servlet可以确定用户文档足够新，这个状态代码是很有用的。
205 Reset Content
没有新文档。但浏览器应该重置它所显示的内容。用来强制浏览器清除表单输入内容。
206 Partial Content
客户发送了一个带有Range头的GET请求，服务器完成了它。



3xx:重定向

300 Multiple Choices
多重选择。链接列表。用户可以选择某链接到达目的地。最多允许五个地址。
301 Moved Permanently
所请求的页面已经转移至新的url。
302 Moved Temporarily
所请求的页面已经临时转移至新的url。
303 See Other
所请求的页面可在别的url下被找到。
304 Not Modified
未按预期修改文档。客户端有缓冲的文档并发出了一个条件性的请求（一般是提供If-Modified-Since头表示客户只想比指定日期更新的文档）。服务器告诉客户，原来缓冲的文档还可以继续使用。
305 Use Proxy
客户请求的文档应该通过Location头所指明的代理服务器提取。
306 Unused
此代码被用于前一版本。目前已不再使用，但是代码依然被保留。
307 Temporary Redirect
被请求的页面已经临时移至新的url。



4xx:客户端错误

400 Bad Request
服务器未能理解请求。
401 Unauthorized
被请求的页面需要用户名和密码。
401.1
登录失败。
401.2
服务器配置导致登录失败。
401.3
由于 ACL 对资源的限制而未获得授权。
401.4
筛选器授权失败。
401.5
ISAPI/CGI 应用程序授权失败。
401.7
访问被 Web 服务器上的 URL 授权策略拒绝。这个错误代码为 IIS 6.0 所专用。
402 Payment Required
此代码尚无法使用。
403 Forbidden
对被请求页面的访问被禁止。
403.1
执行访问被禁止。
403.2
读访问被禁止。
403.3
写访问被禁止。
403.4
要求 SSL。
403.5
要求 SSL 128。
403.6
IP 地址被拒绝。
403.7
要求客户端证书。
403.8
站点访问被拒绝。
403.9
用户数过多。
403.10
配置无效。
403.11
密码更改。
403.12
拒绝访问映射表。
403.13
客户端证书被吊销。
403.14
拒绝目录列表。
403.15
超出客户端访问许可。
403.16
客户端证书不受信任或无效。
403.17
客户端证书已过期或尚未生效。
403.18
在当前的应用程序池中不能执行所请求的 URL。这个错误代码为 IIS 6.0 所专用。
403.19
不能为这个应用程序池中的客户端执行 CGI。这个错误代码为 IIS 6.0 所专用。
403.20
Passport 登录失败。这个错误代码为 IIS 6.0 所专用。
404 Not Found
服务器无法找到被请求的页面。
404.0
没有找到文件或目录。
404.1
无法在所请求的端口上访问 Web 站点。
404.2
Web 服务扩展锁定策略阻止本请求。
404.3
MIME 映射策略阻止本请求。
405 Method Not Allowed
请求中指定的方法不被允许。
406 Not Acceptable
服务器生成的响应无法被客户端所接受。
407 Proxy Authentication Required
用户必须首先使用代理服务器进行验证，这样请求才会被处理。
408 Request Timeout
请求超出了服务器的等待时间。
409 Conflict
由于冲突，请求无法被完成。
410 Gone
被请求的页面不可用。
411 Length Required
"Content-Length" 未被定义。如果无此内容，服务器不会接受请求。
412 Precondition Failed
请求中的前提条件被服务器评估为失败。
413 Request Entity Too Large
由于所请求的实体的太大，服务器不会接受请求。
414 Request-url Too Long
由于url太长，服务器不会接受请求。当post请求被转换为带有很长的查询信息的get请求时，就会发生这种情况。
415 Unsupported Media Type
由于媒介类型不被支持，服务器不会接受请求。
416 Requested Range Not Satisfiable
服务器不能满足客户在请求中指定的Range头。
417 Expectation Failed
执行失败。
423
锁定的错误。



5xx:服务器错误

500 Internal Server Error
请求未完成。服务器遇到不可预知的情况。
500.12
应用程序正忙于在 Web 服务器上重新启动。
500.13
Web 服务器太忙。
500.15
不允许直接请求 Global.asa。
500.16
UNC 授权凭据不正确。这个错误代码为 IIS 6.0 所专用。
500.18
URL 授权存储不能打开。这个错误代码为 IIS 6.0 所专用。
500.100
内部 ASP 错误。
501 Not Implemented
请求未完成。服务器不支持所请求的功能。
502 Bad Gateway
请求未完成。服务器从上游服务器收到一个无效的响应。
502.1
CGI 应用程序超时。　·
502.2
CGI 应用程序出错。
503 Service Unavailable
请求未完成。服务器临时过载或当机。
504 Gateway Timeout
网关超时。
505 HTTP Version Not Supported
服务器不支持请求中指明的HTTP协议版本
```

### 10.http请求

#### 1).请求的网址(Request Body)

![image-20240709191709936](D:/%E6%96%87%E6%A1%A3/%E7%AC%94%E8%AE%B0/%E6%8A%80%E6%9C%AF/%E7%88%AC%E8%99%AB/image-20240709191709936.png)

```
请求的网址即URL，它可以唯一确定请求的资源。
Request URL：部分即为请求的网址（https://www.baidu.com/）。
```

**Content-Type确定了POST请求提交数据的方式，**

| Content-Type                      | 提交数据的方式 |
| --------------------------------- | -------------- |
| application/x-www-form-urlencoded | 表单数据       |
| multipart/form-data               | 表单文件       |
| application/json                  | 序列化JSON数据 |
| text/xml                          | XML数据        |

#### 2).请求方法

常见的请求方法有两种：GET方法和POST方法。

**（1）GET方法，**请求指定的网页信息，并返回网页内容，提交的数据最多只有1024字节。

**（2）POST方法，**向指定资源提交数据并进行请求处理（如提交表单或上传文件）。数据都包含在请求体中，提交的数据没有字节限制。

```
“Request Method”部分即为请求方法（GET方法）。
可以看出，平时打开网站一般使用的是GET方法，也就是请求页面；如果是向网站提交数据（如登录网站），就用到了POST方法。
还有一些其他的请求方法，如HEAD、PUT、DELETE、CONNECT、OPTIONS和TRACE等，在实际编写爬虫程序时很少用到，此处不再介绍
```

#### 3).请求头

```
请求头是请求的重要组成部分，在编写爬虫程序时，大部分情况下都需要设定请求头。不同请求的请求头包含的内容不同，应用时应根据实际需求设定。
“Request Headers”部分即为请求头。
```

| 请求头          | 说 明                                                        |
| --------------- | ------------------------------------------------------------ |
| Accept          | 指定客户端可识别的内容类型                                   |
| Accpet-Encoding | 指定客户端可识别的内容编码                                   |
| Accept-Language | 指定客户端可识别的语言类型                                   |
| Cookie          | 网站为了辨别用户身份进行会话跟踪而存储在用户本地的数据，主要功能是维持当前访问会话 |
| Host            | 指定请求的服务器的域名和端口号                               |
| User-Agent      | 使服务器识别客户端使用的操作系统及版本、浏览器及版本等信息，实现爬虫时加上此信息，可以伪装为浏览器 |
| Content-Type    | 请求的媒体类型信息                                           |
| Content-Length  | 请求的内容长度                                               |
| Referer         | 包含一个URL，用户以该URL代表的页面出发访问当前请求页面       |

#### 4).请求体

```
请求体中的内容一般是POST请求中的表单数据，而GET请求的请求体为空。
例如，使用账户登录百度网站可看到POST请求中的请求体（“Form Data”部分），
```

![image-20240709192336062](D:/%E6%96%87%E6%A1%A3/%E7%AC%94%E8%AE%B0/%E6%8A%80%E6%9C%AF/%E7%88%AC%E8%99%AB/image-20240709192336062.png)

### 11.http响应

```
响应状态码（Response Status Code）
响应头（Response Headers）
响应体（Response Body）
```

#### 1).状态响应码

作用：**表示服务器的响应状态**

| 状态码 | 英文名称              | 说 明                                                      |
| ------ | --------------------- | ---------------------------------------------------------- |
| 100    | Continue              | 服务器已收到请求的一部分，正在等待其余部分，应继续提出请求 |
| 200    | OK                    | 服务器已成功处理了请求                                     |
| 302    | Move Temporarily      | 服务器要求客户端重新发送一个请求                           |
| 304    | Not  Modified         | 此请求返回的网页未修改，继续使用上次的资源                 |
| 404    | Not Found             | 服务器找不到请求的网页                                     |
| 500    | Internal Server Error | 服务器遇到错误，无法完成请求                               |

```
“Status Code”部分即为响应状态码（200 OK）
在爬取网页时，爬虫程序可以根据状态码判断服务器的响应状态，如果状态码为200，则表明返回数据成功，可以进行下一步的处理
```

#### 2).响应头

```
响应头包含了服务器对请求的应答信息
“Response Headers”部分即为响应头
```

| 响应头           | 说 明                    |
| ---------------- | ------------------------ |
| Content-Encoding | Web服务器支持的编码类型  |
| Content-Language | 响应体的语言             |
| Content-Length   | 响应体的长度             |
| Content-Type     | 返回内容的媒体类型       |
| Date             | 原始服务器消息发出的时间 |

| 响应头        | 说 明                     |
| ------------- | ------------------------- |
| Expires       | 响应过期的日期和时间      |
| Last-Modified | 请求资源的最后修改时间    |
| Set-Cookie    | 设置HTTP Cookie           |
| Location      | 重定向接收到请求的URL位置 |

#### 3).响应体

```
响应体包含响应的正文数据。例如，请求网页时，响应体是网页的网页源代码；请求图片时，响应体是图片的二进制数据。
“Response”选项，可显示响应体
```

![image-20240709192933162](D:/%E6%96%87%E6%A1%A3/%E7%AC%94%E8%AE%B0/%E6%8A%80%E6%9C%AF/%E7%88%AC%E8%99%AB/image-20240709192933162.png)

## 二.urllib库

```python
import urllib.request				#导入request模块
url = 'http://fanyi.youdao.com/'			#定义url字符串
#构造HTTP请求，并将返回的结果赋值给response
response = urllib.request.urlopen(url)
print('响应类型：', type(response))			#输出响应类型
print('响应状态码：', response.getcode())		#输出响应状态码
#输出编码方式
print('编码方式：', response.getheader('Content-Type'))
print('请求的URL：', response.geturl())		#输出请求的URL
resp = response.read().decode('utf-8')		#读取网页内容并解码
print('网页内容：\n', resp				#输出网页内容
```

```
urllib.request模块的urlopen()函数获取网页，返回的网页内容数据格式为bytes类型，需要利用decode()函数解码，转换成str类型。
```

```
urllib库是Python内置的标准库，不需要额外安装即可使用，它包含如下四个模块。
    1).request：模拟发送HTTP请求。
    2).error：处理HTTP请求错误时的异常。
    3).parse：解析、拆分和合并URL。
    4).robotparser：解析网站的robots.txt文件。
```

**发送请求**

```
request模块提供了基本的构造HTTP请求的方法，同时它还可以处理授权验证（authentication）、重定向（redirection）、Cookie会话及其他内容。
```

#### 1).urlopen()函数

urlopen()函数可以构造基本的HTTP请求

```python
urlopen(url,data=None,[timeout,]*,cafile=None,capath=None,
cadefault=False,context=None)

1.url：请求的URL。可以是一个表示URL的字符串，也可以是一个Request类型的对象。这是必传参数，其他都是可选参数。
2.data：请求体信息（如在线翻译，在线答题等提交的内容）。data默认值是None，表示以GET方式发送请求；当用户给出data参数时，表示以POST方式发送请求。
3.timeout：设置网站的访问超时时间，单位为秒。如果请求超出了设置的时间而没有得到响应，就会抛出异常。如果不指定该参数，就会使用全局默认时间。
4.cafile、capath、cadefault：用于实现可信任CA证书的HTTP请求，基本很少使用。
5.context：实现SSL加密传输，基本很少使用。
```

Request类型的对象是调用urllib.request.Request()函数返回的对象，见Request()函数部分。

urlopen()函数的data参数必须是bytes（字节流）类型，如果是字典类型，可以先用urllib.parse模块的urlencode()函数编码，见2.2.5小节。

调用函数后，返回一个HTTPResponse类型的对象

| 方法和属性        | 说 明                          |
| ----------------- | ------------------------------ |
| getcode()/status  | 获取响应状态码                 |
| get_url()         | 获取请求的URL                  |
| getheaders()      | 获取响应头信息，返回二元组列表 |
| getheader(name)   | 获取特定响应头信息             |
| info()            | 获取响应头信息，返回字符串     |
| read()/readline() | 读取响应体                     |

```python
import urllib.request		#导入request模块
url = 'http://fanyi.youdao.com/translate?smartresult=dict&' \
      'smartresult=rule'		#定义url字符串
datavalue = {
    'i': '苹果',
    'from': 'AUTO',
    'to': 'AUTO',
    'smartresult': 'dict',
    'client': 'fanyideskweb',
    'salt': '15924715113269',
    'sign': '5c3a992ac57ed879b7678ff05bb3ec44',
    'ts': '1592471511326',
    'bv': 'c74c03c52496795b65595fdc27140f0f',
    'doctype': 'json',
    'version': '2.1',
    'keyfrom': 'fanyi.web',
    'action': 'FY_BY_REALTlME'
}					#定义datavalue参数
#编码datavalue参数
datavalue = urllib.parse.urlencode(datavalue).encode('utf-8')
#构造HTTP请求，并将返回的结果赋值给response
response = urllib.request.urlopen(url, data=datavalue)
resp = response.read().decode('utf-8')	#读取网页内容并解码
print(resp)				#输出网页内容
```

```
有道在线翻译网站存在反爬虫机制，在编写爬虫程序时需要将url中的“_o”去掉。
程序的运行结果如图2-7所示。返回的响应体中包含了翻译的内容和结果，如果想要修改翻译的内容，只需要修改data参数中“i”的值
```

#### 2).Request()函数

当HTTP请求信息较复杂时，可用Request()函数进行设置，其函数原型如下：

```python
Request(url,data=None,headers={},origin_req_host=None,
unverifiable=False, method=None)

1）url：请求的URL。
2）data：请求体信息，其使用方法与urlopen()函数中的data参数相同。
3）headers：请求头信息，如User_Agent、Cookie和Host等，是字典类型。
4）origin_req_host：客户端的host名称或者IP地址。
5）unverifiable：表示这个请求是无法验证的，在默认情况下设置为False。
6）method：请求方法，如GET、POST等，是字符串类型。
调用函数后，返回一个Request类型的对象，然后再通过urlopen()函数构造完整的HTTP请求。
当服务器有反爬虫机制时，可通过设置headers参数伪装成浏览器去访问网站。
```

#### 3).处理异常

```
error模块提供了request模块产生的异常处理方法，它主要包含了URLError和HTTPError两个类。
（1）URLError类是error异常模块的基类，可以捕获request模块产生的异常，它具有一个reason属性（返回异常的原因）。
（2）HTTPError类是URLError类的子类，专门处理HTTP请求的异常，它具有3个属性，分别为reason（返回异常原因）、code（返回HTTP状态码）和headers（返回请求头）。
因为HTTPError是URLError的子类，并不能处理父类支持的异常处理，所以一般对两种异常分开捕获，可先捕获子类的异常，再捕获父类的异常。
```

#### 4).解析URL

parse模块提供了解析URL的方法，包括URL的拆分、合并和转换，

| 功 能 | 函 数               | 说 明                                                        |
| ----- | ------------------- | ------------------------------------------------------------ |
| 拆分  | urlparse(urlstring) | 将URL拆分为六个部分，分别是scheme、netloc、path、params、query和fragment |
| 拆分  | urlsplit(urlstring) | 将URL拆分为五个部分，分别是scheme、netloc、path、query和fragment |

| 功 能 | 函 数              | 说 明                                                        |
| ----- | ------------------ | ------------------------------------------------------------ |
| 合并  | urljoin(url1,url2) | 将基础链接url1和新链接url2合并，分析url1的scheme、netloc、path内容，并补充url2缺失的部分 |
| 合并  | urlunparse(parts)  | 将可迭代对象parts合并为URL，parts长度为7                     |
| 合并  | urlunsplit(parts)  | 将可迭代对象parts合并为URL，parts长度为6                     |
| 转换  | urlencode(query)   | 将字典形式的数据转换为URL后面的查询字符串                    |
| 转换  | parse_qs(qs)       | 将URL后面的查询字符串转换为字典                              |
| 转换  | parse_qsl(qs)      | 将URL后面的查询字符串转换为列表                              |
| 转换  | quote(str)         | 将URL中的中文字符转换为URL编码                               |
| 转换  | unquote(str)       | 将URL编码转换为中文字符，进行解码                            |

#### 5).分析Robots协议

robotparser模块提供了分析网站Robots协议的RobotFileParser类，它可以通过分析网站的robots.txt文件来判断某网页是否能被爬取。RobotFileParser类提供了多种方法，

常用的方法如下。

```
1）set_url()：设置robots.txt文件的URL。
2）read()：读取robots.txt文件并进行分析。
3）can_fetch()：第一个参数为User_Agent，第二个参数为要爬取网页的URL，判断该网页是否能被爬取。
4）parse()：解析robots.txt文件中某些行的内容。
5）mtime()：返回上次抓取和分析robots.txt文件的时间。
6）modified()：将当前时间设置为上次抓取和分析robots.txt文件的时间。
```

## 三.requests库

```
requests库提供了几乎所有的HTTP请求方法，其中最常用的是GET方法和POST方法。
```

### 1.GET方法

urlopen()函数可以构造基本的HTTP请求，其函数原型如下：

```python
get(url，params=None,**kwargs)

1）url：请求的URL。这是必传参数，其他都是可选参数。
2）params：字典或字节序列，作为参数增加到url中。
3）**kwargs：控制访问的参数，如headers、cookies、timeout和proxies等。
```

调用函数后，返回一个Response类型的对象

| 属性/方法          | 说  明                                                       |
| ------------------ | ------------------------------------------------------------ |
| status_code        | 获取响应状态码                                               |
| headers            | 获取响应头                                                   |
| request.headers    | 获取请求头                                                   |
| url                | 获取请求的URL                                                |
| encoding           | 获取从HTTP  headers中猜测的响应内容编码方式                  |
| apparent_encoding  | 获取从响应内容分析出的编码方式                               |
| content            | 获取二进制类型的响应内容，会自动解码gzip和deflate编码的响应内容 |
| text               | 获取文本类型的响应内容                                       |
| json()             | 返回JSON类型数据                                             |
| raise_for_status() | 若是status_code不是200，则会抛出异常                         |

### 2.POST方法

POST方法通过post()函数实现，其函数原型如下：

```python
post(url,data=None,json=None,**kwargs)

1）url：请求的URL。这是必传参数，其他都是可选参数。
2）data：字典、字节序列或文件对象，作为请求体的内容。
3）json：JSON格式的数据，作为请求体的内容。
4）**kwargs：控制访问的参数，如params、headers、cookies、timeout和proxies等。
post()函数同样返回一个Response类型的对象。
```

Form Data部分为提交的表单数据，其中，name为账户名，password为密码，remember为是否记住密码，false表示不记住密码。

```
requests库实现HTTP其他基本请求的方法如下：
requests.put('https://www.douban.com/')	#PUT请求
requests.delete('https://www.douban.com/')	#DELETE请求
requests.head('https://www.douban.com/')	#HEAD请求
requests.options('https://www.douban.com/')	#OPTIONS请求
```

### 3.传递URL参数

有时网站会通过URL来传递查询参数，这时可使用get()函数或post()函数的params参数进行设置。

### 4.定制请求头

在requests库中，get()或post()函数可以直接传递字典形式的User_Agent信息给headers参数实现定制请求头。

![image-20240709200023079](D:/%E6%96%87%E6%A1%A3/%E7%AC%94%E8%AE%B0/%E6%8A%80%E6%9C%AF/%E7%88%AC%E8%99%AB/image-20240709200023079.png)

```
第一次发送HTTP请求时，请求头中的User_Agent信息为Python的requests库的版本信息，表示爬虫程序发送该请求，且返回状态码是418，表示禁止爬虫程序爬取该网站；
第二次设置为浏览器信息，返回状态码为200，表示伪装成浏览器后成功爬取网站。所以，设置请求头参数是应对反爬虫机制的一种有效方法。
```

### 5.设置Cookie

```
利用post()函数登录了豆瓣网页，但是如果想要爬取豆瓣网站登录后的内容，还需要用到get()函数去请求页面，这时候可直接利用Cookie来维持登录状态。

Cookie可以帮助用户记录访问Web页面时的个人信息，并保存在客户端，当用户再次访问同一Web页面时，Cookie可以提供上次用户访问信息。

Cookie还可以通过get()函数或post()函数的cookies参数进行设置。
```

### 6.设置超时

```
在爬取网页的过程中，有时服务器没有响应，程序可能会一直等待响应，requests库可以在get()或post()函数中设置timeout参数来解决这个问题。程序在等待设置的秒数后会停止等待，抛出Timeout异常。

使用requests库发出HTTP请求时可能会发生异常，如错误异常（requests.HTTPError）、URL缺失异常（requests.URLRequired）和请求URL超时异常（requests.Timeout）等。Response类型的对象还提供了raise_for_status()方法来捕获访问网页后的HTTP响应状态码不是200时产生的requests.HTTPError。
```

### 7.获取二进制文件

```
图片、音频、视频等文件本质上是由二进制码组成的，有特定的保存格式和对应的解码方式，想要爬取这些文件，需要获取它们的二进制数据。

https://img9.doubanio.com/view/subject/s/public/s33643895.jpg
```

```python
import requests		#导入requests模块
r=requests.get('https://img9.doubanio.com/view/subject/s/public/s33643895.jpg')			#发送HTTP请求，并将返回结果赋值给r
print(r.content) 				#输出二进制类型返回内容
print(r.text) 				#输出文本类型返回内容
#保存二进制类型返回内容为jpg文件
with open('fengmian.jpg', 'wb') as f:
    f.write(r.content)
```

![image-20240709200724397](D:/%E6%96%87%E6%A1%A3/%E7%AC%94%E8%AE%B0/%E6%8A%80%E6%9C%AF/%E7%88%AC%E8%99%AB/image-20240709200724397.png)

```
“content”输出的结果前带有一个“b”，表示数据是bytes类型；
“text”输出的结果是乱码，这是因为图片是二进制数据，不能直接将其转化为字符串。
代码最后将图片以二进制形式保存了下来，程序运行结束后，文件夹中将出现名为“fengmian.jpg”的图片文件，打开的图片如图2-25所示
```

## 四.字符编码

```
在Python 3中，字符串使用str或bytes类型。str类型默认为Unicode编码格式，该编码格式不适合用来传输和存储，所以需要将str类型进行编码，转换成bytes类型（如UTF-8、GBK等格式）。在做编码转换时，通常需要以Unicode作为中间编码，
即先将其他编码的字符串解码（decode()方法）成Unicode，再从Unicode编码（encode()方法）成另一种编码。
如果要查看具体的编码类型，可以使用chardet模块中的detect()方法。
```

Unicode不能通过decode()方法再次进行解码，已经编码的内容也不能通过encode()方法再次进行编码。

# 二、解析数据

## 一.网页基础

### 1.请求的网址

网页一般由HTML、CSS和JavaScript三部分组成，其中HTML用于定义网页的结构和内容；CSS用于定义网页的样式；JavaScript用于定义网页的行为。

超文本标记语言（Hyper Text Marked Language，HTML）是一种用来描述网页的语言。它通过不同类型的标签来描述不同的元素，各种标签通过不同的排列和嵌套形成网页的框架。有的标签还带有属性参数，其语法格式如下：

```html
＜标签  属性="参数值"＞
```

| 标 签       | 说 明                                                   |
| ----------- | ------------------------------------------------------- |
| <！DOCTYPE> | 定义文档类型                                            |
| <html>      | 定义HTML文档，标记符是<html></html>                     |
| <head>      | 定义文档的头部，标记符是<head></head>                   |
| <meta>      | 定义关于HTML文档的元信息，标记符是<meta  属性="参数值"> |
| <title>     | 定义文档的标题，标记符是<title>文档标题</title>         |
| <body>      | 定义文档的主体，标记符是<body></body>                   |
| <div>       | 定义文档中的节，标记符是<div></div>                     |
| <ul>        | 定义无序列表，标记符是<ul></ul>                         |

| 标 签      | 说 明                                                        |
| ---------- | ------------------------------------------------------------ |
| <li>       | 定义列表项目，标记符是<li></li>                              |
| <h1>～<h6> | 定义HTML标题，<h1>定义最大的标题，<h6>定义最小的标题，标记符是<h1>我的标题</h1> |
| <p>        | 定义段落，标记符是<p></p>                                    |
| <a>        | 定义超链接目标，标记符是<a  href="地址">链接名称</a>         |
| <link>     | 定义文档与外部资源的关系，标记符是<link  属性="参数值">      |
| <script>   | 定义客户端脚本，标记符是<script></script>                    |

### 2.html DOM

文档对象模型（Document Object Model，DOM）定义了访问HTML和可扩展标记语言（Extensible Markup Language，XML）文档的标准。HTML DOM将HTML文档呈现为带有元素、属性和文本的树结构（也称为节点树）

![image-20240709201443173](D:/%E6%96%87%E6%A1%A3/%E7%AC%94%E8%AE%B0/%E6%8A%80%E6%9C%AF/%E7%88%AC%E8%99%AB/image-20240709201443173.png)

在节点树中，顶端节点称为根（root），除了根节点外，其他每个节点都有父节点（parent），同时可拥有任意数量的子节点（child）和兄弟节点（sibling）。例如，根元素<html>、元素<head>和元素<body>相互之间的关系可用图3-3表示。

![image-20240709201500174](D:/%E6%96%87%E6%A1%A3/%E7%AC%94%E8%AE%B0/%E6%8A%80%E6%9C%AF/%E7%88%AC%E8%99%AB/image-20240709201500174.png)

### 3.CSS选择器

层叠样式表（Cascading Style Sheets，CSS）选择器可以定位节点

| 选择器          | 示 例      | 示例说明                           |
| --------------- | ---------- | ---------------------------------- |
| .class          | .intro     | 选择class="intro"的所有节点        |
| #id             | #firstname | 选择id="firstname"的所有节点       |
| *               | *          | 选择所有节点                       |
| element         | p          | 选择所有<p>节点                    |
| element,element | div,p      | 选择所有<div>节点和所有<p>节点     |
| element element | div p      | 选择<div>节点内部的所有<p>节点     |
| element>element | div>p      | 选择父节点为<div>节点的所有<p>节点 |

| [attribute]        | [target]        | 选择带有target属性的所有节点                |
| ------------------ | --------------- | ------------------------------------------- |
| [attribute=value]  | [target=blank]  | 选择target="blank"的所有节点                |
| :link              | a:link          | 选择所有未访问的链接                        |
| :visited           | a:visited       | 选择所有已访问的链接                        |
| :active            | a:active        | 选择活动链接                                |
| :first-line        | p:first-line    | 选择每个<p>节点的首行                       |
| element1~element2  | p~ul            | 选择前面有<p>节点的所有<ul>节点             |
| [attribute^=value] | a[src^="https"] | 选择其src属性值以“https”开头的所有<a>节点   |
| [attribute$=value] | a[src$=".pdf"]  | 选择其src属性值以“.pdf”结尾的所有<a>节点    |
| [attribute*=value] | a[src*="abc"]   | 选择其src属性值中包含“abc”子串的所有<a>节点 |
| :enabled           | input:enabled   | 选择每个启用的<input>节点                   |
| :disabled          | input:disabled  | 选择每个禁用的<input>节点                   |
| :checked           | input:checked   | 选择每个被选中的<input>节点                 |

## 二.lxml

有同学说，我正则用的不好，处理HTML文档很累，有没有其他的方法？

有！那就是XPath，我们可以先将 HTML文件 转换成 XML文档，然后用 XPath 查找 HTML 节点或元素。

### 1.什么是XML

- XML 指可扩展标记语言（EXtensible Markup Language）
- XML 是一种标记语言，很类似 HTML
- XML 的设计宗旨是传输数据，而非显示数据
- XML 的标签需要我们自行定义。
- XML 被设计为具有自我描述性。
- XML 是 W3C 的推荐标准

W3School官方文档：http://www.w3school.com.cn/xml/index.asp

#### a.XML 和 HTML 的区别

| 数据格式 |                      描述                       | 设计目标                                                     |
| -------- | :---------------------------------------------: | ------------------------------------------------------------ |
| XML      | Extensible Markup Language `（可扩展标记语言）` | 被设计为传输和存储数据，其焦点是数据的内容。                 |
| HTML     | HyperText Markup Language `（超文本标记语言）`  | 显示数据以及如何更好显示数据。                               |
| HTML DOM | Document Object Model for HTML `(文档对象模型)` | 通过 HTML DOM，可以访问所有的 HTML 元素，连同它们所包含的文本和属性。可以对其中的内容进行修改和删除，同时也可以创建新的元素。 |

#### b.XML文档示例

```xml
<?xml version="1.0" encoding="utf-8"?>

<bookstore>

  <book category="cooking">
    <title lang="en">Everyday Italian</title>  
    <author>Giada De Laurentiis</author>  
    <year>2005</year>  
    <price>30.00</price>
  </book>  

  <book category="children">
    <title lang="en">Harry Potter</title>  
    <author>J K. Rowling</author>  
    <year>2005</year>  
    <price>29.99</price>
  </book>  

  <book category="web">
    <title lang="en">XQuery Kick Start</title>  
    <author>James McGovern</author>  
    <author>Per Bothner</author>  
    <author>Kurt Cagle</author>  
    <author>James Linn</author>  
    <author>Vaidyanathan Nagarajan</author>  
    <year>2003</year>  
    <price>49.99</price>
  </book>

  <book category="web" cover="paperback">
    <title lang="en">Learning XML</title>  
    <author>Erik T. Ray</author>  
    <year>2003</year>  
    <price>39.95</price>
  </book>

</bookstore>
```

#### c.HTML DOM 模型示例

HTML DOM 定义了访问和操作 HTML 文档的标准方法，以树结构方式表达 HTML 文档。

##### 1. 父（Parent）

每个元素以及属性都有一个父。

下面是一个简单的XML例子中，book 元素是 title、author、year 以及 price 元素的父：

```xml
<?xml version="1.0" encoding="utf-8"?>
<book>
  <title>Harry Potter</title>
  <author>J K. Rowling</author>
  <year>2005</year>
  <price>29.99</price>
</book>
```

##### 2. 子（Children）

元素节点可有零个、一个或多个子。

在下面的例子中，title、author、year 以及 price 元素都是 book 元素的子：

```xml
<?xml version="1.0" encoding="utf-8"?>
<book>
  <title>Harry Potter</title>
  <author>J K. Rowling</author>
  <year>2005</year>
  <price>29.99</price>
</book>
```

##### 3. 同胞（Sibling）

拥有相同的父的节点

在下面的例子中，title、author、year 以及 price 元素都是同胞：

```xml
<?xml version="1.0" encoding="utf-8"?>
<book>
  <title>Harry Potter</title>
  <author>J K. Rowling</author>
  <year>2005</year>
  <price>29.99</price>
</book>
```

##### 4. 先辈（Ancestor）

某节点的父、父的父，等等。

在下面的例子中，title 元素的先辈是 book 元素和 bookstore 元素：

```xml
<?xml version="1.0" encoding="utf-8"?>
<bookstore>
<book>
  <title>Harry Potter</title>
  <author>J K. Rowling</author>
  <year>2005</year>
  <price>29.99</price>
</book>

</bookstore>
```

##### 5. 后代（Descendant）

某个节点的子，子的子，等等。

在下面的例子中，bookstore 的后代是 book、title、author、year 以及 price 元素：

```xml
<?xml version="1.0" encoding="utf-8"?>

<bookstore>

<book>
  <title>Harry Potter</title>
  <author>J K. Rowling</author>
  <year>2005</year>
  <price>29.99</price>
</book>

</bookstore>
```

### 2.什么是XPath？

> XPath (XML Path Language) 是一门在 XML 文档中查找信息的语言，可用来在 XML 文档中对元素和属性进行遍历。

#### 1.概念

XML路径语言（XML path language，XPath）用于在XML文档中查找信息，同样适用于HTML文档。

lxml库的大部分功能是由etree模块提供的。使用XPath解析网页时，首先需要调用etree模块下的HTML类对HTTP响应的网页进行初始化（etree.HTML()），从而构造一个Element类型的XPath解析对象；然后使用XPath对Element对象进行节点选择，最后返回一个列表。若HTML中的节点没有闭合，etree模块还可提供自动补全功能。

#### 2.语法

```
XPath的选择功能十分强大，它提供了非常简明的路径选择表达式。另外，它还提供了超过100个内建函数，用于字符串、数值、时间的匹配及节点、序列的处理等。可以说，几乎所有想要定位的节点，都可以用XPath来选择。
```

##### 1).通过路径选择节点

```
HTML源代码是层次结构的，如果想要选择一个节点，可以一层一层往下查找。XPath实际上就是使用这种层次结构的路径来找到相应的节点，它类似于人们日常使用的地址，它们都是从大的范围一直缩小到具体的某个地址。XPath通过路径选择节点常用的语法
```

| 语 法 | 示 例       | 示例说明                                     |
| ----- | ----------- | -------------------------------------------- |
| /     | body/div    | 选取body节点下的所有div子节点                |
| //    | body//div   | 选取body节点下的所有div子孙节点              |
| *     | body/*      | 选取body节点下的所有子节点                   |
| ．    | body/．     | 选取当前body节点                             |
| ．．  | body/．．   | 选取当前body节点的父节点                     |
| [ ]   | body/div[1] | 选取body节点下的第一个div子节点，下标从1开始 |

##### 2).通过属性选择节点

```
XPath可以通过节点的属性来选择包含某个指定属性值的节点。它可在仅掌握节点部分特征的情况下，利用模糊搜索函数选择节点。XPath通过属性选择节点常用的语法
```

| 语 法         | 示 例                                    | 示例说明                                                 |
| ------------- | ---------------------------------------- | -------------------------------------------------------- |
| @             | //div[@class="content"]                  | 选取class属性值为“content”的div节点                      |
| starts-with() | //div[starts-with(@class,"con")]         | 选取class属性值以“con”开头的div节点                      |
| contains()    | //div[contains(@class,"tent")]           | 选取class属性值包含“tent”的div节点                       |
| and           | //div[@id="content"and @class="showtxt"] | 选取id属性值为“content”和class属性值为“showtxt”的div节点 |

```
@也可以用于提取节点的属性值。例如，//div[@id="content"]/@class表示提取id属性值为“content”的div节点的class属性值。
```

##### 3).提取文本

```
解析网页的目的是通过选择节点来提取节点文本或属性值，从而获取所需的信息。XPath提取文本的方法
```

| 方  法 | 示  例      | 示例说明                                  |
| ------ | ----------- | ----------------------------------------- |
| text() | //a/text()  | 提取所有a节点的文本，返回一个列表         |
| ()     | string(//a) | 提取a节点下所有节点的文本，返回一个字符串 |

```
text()方法也可用于选取节点文本包含指定值的节点，如//a[contains(text(),'Python']表示选取文本包含“Python”的a节点。

GB2312编码支持的汉字比较少，使用这种编码类型时，中文内容会出现小部分乱码，而GBK编码向下兼容GB2312编码，因此如果检测到的编码类型是GB2312，可直接将编码类型改为GBK。
```

##### 4).XPath 开发工具

1. 开源的XPath表达式编辑工具:XMLQuire(XML格式文件可用)
2. Chrome插件 XPath Helper
3. Firefox插件 XPath Checker

##### 5).选取节点

XPath 使用路径表达式来选取 XML 文档中的节点或者节点集。这些路径表达式和我们在常规的电脑文件系统中看到的表达式非常相似。

下面列出了最常用的路径表达式：

| 表达式   | 描述                                                       |
| -------- | ---------------------------------------------------------- |
| nodename | 选取此节点的所有子节点。                                   |
| /        | 从根节点选取。                                             |
| //       | 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。 |
| .        | 选取当前节点。                                             |
| ..       | 选取当前节点的父节点。                                     |
| @        | 选取属性。                                                 |

在下面的表格中，我们已列出了一些路径表达式以及表达式的结果：

|                 | 路径表达式                                                   | 结果 |
| --------------- | ------------------------------------------------------------ | ---- |
| bookstore       | 选取 bookstore 元素的所有子节点。                            |      |
| /bookstore      | 选取根元素 bookstore。注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！ |      |
| bookstore/book  | 选取属于 bookstore 的子元素的所有 book 元素。                |      |
| //book          | 选取所有 book 子元素，而不管它们在文档中的位置。             |      |
| bookstore//book | 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。 |      |
| //@lang         | 选取名为 lang 的所有属性。                                   |      |

##### 6).谓语（Predicates）

谓语用来查找某个特定的节点或者包含某个指定的值的节点，被嵌在方括号中。

在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果：

| 路径表达式                         | 结果                                                         |
| ---------------------------------- | ------------------------------------------------------------ |
| /bookstore/book[1]                 | 选取属于 bookstore 子元素的第一个 book 元素。                |
| /bookstore/book[last()]            | 选取属于 bookstore 子元素的最后一个 book 元素。              |
| /bookstore/book[last()-1]          | 选取属于 bookstore 子元素的倒数第二个 book 元素。            |
| /bookstore/book[position()<3]      | 选取最前面的两个属于 bookstore 元素的子元素的 book 元素。    |
| //title[@lang]                     | 选取所有拥有名为 lang 的属性的 title 元素。                  |
| //title[@lang=’eng’]               | 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。   |
| /bookstore/book[price>35.00]       | 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。 |
| /bookstore/book[price>35.00]/title | 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。 |

##### 7).选取未知节点

XPath 通配符可用来选取未知的 XML 元素。

| 通配符 | 描述                 |
| ------ | -------------------- |
| *      | 匹配任何元素节点。   |
| @*     | 匹配任何属性节点。   |
| node() | 匹配任何类型的节点。 |

在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：

| 路径表达式          | 结果                                       |
| ------------------- | ------------------------------------------ |
| /bookstore/*        | 选取 bookstore 元素的所有子元素。          |
| //*                 | 选取文档中的所有元素。                     |
| html/node()/meta/@* | 选择html下面任意节点下的meta节点的所有属性 |
| //title[@*]         | 选取所有带有属性的 title 元素。            |

##### 8).选取若干路径

通过在路径表达式中使用“|”运算符，您可以选取若干个路径。

在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：

| 路径表达式                       | 结果                                                         |
| -------------------------------- | ------------------------------------------------------------ |
| //book/title \| //book/price     | 选取 book 元素的所有 title 和 price 元素。                   |
| //title \| //price               | 选取文档中的所有 title 和 price 元素。                       |
| /bookstore/book/title \| //price | 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。 |

#### 3.lxml库

> lxml 是 一个HTML/XML的解析器，主要的功能是如何解析和提取 HTML/XML 数据。
>
> lxml和正则一样，也是用 C 实现的，是一款高性能的 Python HTML/XML 解析器，我们可以利用之前学习的XPath语法，来快速的定位特定元素以及节点信息。
>
> lxml python 官方文档：http://lxml.de/index.html
>
> 需要安装C语言库，可使用 pip 安装：`pip install lxml` （或通过wheel方式安装）

##### a.初步使用

我们利用它来解析 HTML 代码，简单示例：

```python
# lxml_test.py
# 使用 lxml 的 etree 库
from lxml import etree
text = '''
<div>
    <ul>
         <li class="item-0"><a href="link1.html">first item</a></li>
         <li class="item-1"><a href="link2.html">second item</a></li>
         <li class="item-inactive"><a href="link3.html">third item</a></li>
         <li class="item-1"><a href="link4.html">fourth item</a></li>
         <li class="item-0"><a href="link5.html">fifth item</a> # 注意，此处缺少一个 </li> 闭合标签
     </ul>
 </div>
'''
#利用etree.HTML，将字符串解析为HTML文档
html = etree.HTML(text)
# 按字符串序列化HTML文档
result = etree.tostring(html)
print(result)
```

输出结果：

```html
<html><body>
<div>
    <ul>
         <li class="item-0"><a href="link1.html">first item</a></li>
         <li class="item-1"><a href="link2.html">second item</a></li>
         <li class="item-inactive"><a href="link3.html">third item</a></li>
         <li class="item-1"><a href="link4.html">fourth item</a></li>
         <li class="item-0"><a href="link5.html">fifth item</a></li>
</ul>
 </div>
</body></html>
```

lxml 可以自动修正 html 代码，例子里不仅补全了 li 标签，还添加了 body，html 标签。

##### b.文件读取

除了直接读取字符串，lxml还支持从文件里读取内容。我们新建一个hello.html文件：

```html
<!-- hello.html -->

<div>
    <ul>
         <li class="item-0"><a href="link1.html">first item</a></li>
         <li class="item-1"><a href="link2.html">second item</a></li>
         <li class="item-inactive"><a href="link3.html"><span class="bold">third item</span></a></li>
         <li class="item-1"><a href="link4.html">fourth item</a></li>
         <li class="item-0"><a href="link5.html">fifth item</a></li>
     </ul>
 </div>
```

再利用 etree.parse() 方法来读取文件。

```python
# lxml_parse.py

from lxml import etree

# 读取外部文件 hello.html
html = etree.parse('./hello.html')
result = etree.tostring(html, pretty_print=True)

print(result)
```

输出结果与之前相同：

```html
<html><body>
<div>
    <ul>
         <li class="item-0"><a href="link1.html">first item</a></li>
         <li class="item-1"><a href="link2.html">second item</a></li>
         <li class="item-inactive"><a href="link3.html">third item</a></li>
         <li class="item-1"><a href="link4.html">fourth item</a></li>
         <li class="item-0"><a href="link5.html">fifth item</a></li>
</ul>
 </div>
</body></html>
```

##### c.XPath实例测试

###### 1. 获取所有的 `<li>` 标签

```python
# xpath_li.py

from lxml import etree

html = etree.parse('hello.html')
print type(html)  # 显示etree.parse() 返回类型

result = html.xpath('//li')

print result  # 打印<li>标签的元素集合
print len(result)
print type(result)
print type(result[0])
```

输出结果：

```python
<type 'lxml.etree._ElementTree'>
[<Element li at 0x1014e0e18>, <Element li at 0x1014e0ef0>, <Element li at 0x1014e0f38>, <Element li at 0x1014e0f80>, <Element li at 0x1014e0fc8>]
5
<type 'list'>
<type 'lxml.etree._Element'>
```

###### 2. 继续获取`<li>` 标签的所有 `class`属性

```python
# xpath_li.py

from lxml import etree

html = etree.parse('hello.html')
result = html.xpath('//li/@class')

print result
```

运行结果

```c
['item-0', 'item-1', 'item-inactive', 'item-1', 'item-0']
```

###### 3. 继续获取`<li>`标签下`hre` 为 `link1.html` 的 `<a>` 标签

```python
# xpath_li.py

from lxml import etree

html = etree.parse('hello.html')
result = html.xpath('//li/a[@href="link1.html"]')

print result
```

运行结果

```
[<Element a at 0x10ffaae18>]
```

###### 4. 获取`<li>` 标签下的所有 `<span>` 标签

```python
# xpath_li.py

from lxml import etree

html = etree.parse('hello.html')

#result = html.xpath('//li/span')
#注意这么写是不对的：
#因为 / 是用来获取子元素的，而 <span> 并不是 <li> 的子元素，所以，要用双斜杠

result = html.xpath('//li//span')

print result
```

运行结果

```
[<Element span at 0x10d698e18>]
```

###### 5. 获取 `<li>` 标签下的`<a>`标签里的所有 class

```python
# xpath_li.py

from lxml import etree

html = etree.parse('hello.html')
result = html.xpath('//li/a//@class')

print result
```

运行结果

```
['blod']
```

###### 6. 获取最后一个 `<li>` 的 `<a>` 的 href

```python
# xpath_li.py

from lxml import etree

html = etree.parse('hello.html')

result = html.xpath('//li[last()]/a/@href')
# 谓语 [last()] 可以找到最后一个元素

print result
```

运行结果

```
['link5.html']
```

###### 7. 获取倒数第二个元素的内容

```python
# xpath_li.py

from lxml import etree

html = etree.parse('hello.html')
result = html.xpath('//li[last()-1]/a')

# text 方法可以获取元素内容
print result[0].text
```

运行结果

```
fourth item
```

###### 8. 获取 `class` 值为 `bold` 的标签名

```python
# xpath_li.py

from lxml import etree

html = etree.parse('hello.html')

result = html.xpath('//*[@class="bold"]')

# tag方法可以获取标签名
print result[0].tag
```

运行结果

```
span
```

## 三.beautifulsoup4

beautifulsoup4库也称为Beautiful Soup库或bs4库，用于解析HTML或XML文档。beautifulsoup4库不是Python内置的标准库，使用之前需要安装，安装方法与requests库的安装类似（请参考2.3.2小节），此处不再赘述。

beautifulsoup4库在解析网页时需要依赖解析器，它支持Python标准库中的HTML解析器，还支持一些其他的第三方解析器

beautifulsoup4库可以自动将输入文档转换为Unicode编码，将输出文档转换为UTF-8编码，故不需要考虑编码方式。

| 解析器                     | 使用方法                            | 优 点                                                   | 缺 点                                    |
| -------------------------- | ----------------------------------- | ------------------------------------------------------- | ---------------------------------------- |
| Python标准库中的HTML解析器 | BeautifulSoup(markup,"html.parser") | Python的内置标准库，执行速度适中，文档容错能力强        | Python2.7.3或3.2.2前的版本文档容错能力差 |
| lxml  HTML解析器           | BeautifulSoup(markup,"lxml")        | 速度快，文档容错能力强                                  | 需要安装C语言库                          |
| lxml  XML解析器            | BeautifulSoup(markup,"xml")         | 速度快，唯一支持XML的解析器                             | 需要安装C语言库                          |
| html5lib                   | BeautifulSoup(markup,"html5lib")    | 容错性最好，以浏览器的方式解析文档，生成HTML5格式的文档 | 速度慢，不依赖外部扩展                   |

### 1.语法

```
beautifulsoup4库中最重要的是BeautifulSoup类，它的实例化对象相当于一个页面。解析网页时，需要使用BeautifulSoup()创建一个BeautifulSoup对象，该对象是一个树形结构，包含了HTML页面中的标签元素，如<head>、<body>等。也就是说，HTML中的主要结构都变成了BeautifulSoup对象的一个个属性，然后可通过“对象名.属性名”形式获取该对象的第一个属性值（即节点）。

BeautifulSoup对象的属性名与HTML的标签名相同
```

| 属 性    | 说 明                                                        |
| -------- | ------------------------------------------------------------ |
| name     | 标签的名字，如head、title等，返回一个字符串                  |
| string   | 标签所包围的文字，网页中真实的文字（尖括号之间的内容），返回一个字符串 |
| attrs    | 字典，包含了页面标签的所有属性（尖括号内的所有项），如href，返回一个字典 |
| contents | 这个标签下所有子标签的内容，返回一个列表                     |

```
其中，attrs返回的是标签的所有属性组成的字典类型的数据，可通过“atrrs['属性名']”形式获取属性值。

按照HTML语法，可以在标签中嵌套其他标签，因此string属性的返回值遵循如下原则。
    （1）如果标签内部没有其他标签，string属性返回其中的内容。
    （2）如果标签内部还有其他标签，但只有一个标签，string属性返回最里面标签的内容。
    （3）如果标签内部还有其他标签，且不止一个标签，string属性返回None。
```

### 2.方法选择器

beautifulsoup4库还提供了一些查询方法，如find_all()和find()等。

find_all()方法会遍历整个HTML文件，按照条件返回所有匹配的节点，其方法原型如下：

```
find_all(name, attrs, recursive, string, limit)

（1）name：通过HTML标签名直接查找节点。
（2）attrs：通过HTML标签的属性查找节点（需列出属性名和值），可以同时设置多个属性。
（3）recursive：搜索层次，默认查找当前标签的所有子孙节点，如果只查找标签的子节点，可以使用参数recursive = False。
（4）string：通过关键字检索string属性内容，传入的形式可以是字符串，也可以是正则表达式对象。
（5）limit：返回结果的个数，默认返回全部结果。
简单地说，BeautifulSoup对象的find_all()方法可以根据标签名、标签属性和内容，查找并返回节点列表。
```

find()方法的用法与find_all()方法类似，但其返回结果是第一个匹配的节点。

```
find_all()方法通过属性查找节点时，对于一些常用的属性（如id和class等），可以不用attrs字典形式来传递，而用赋值的形式直接传入参数（如find_all(class_='hotnews')，由于class在Python中是一个关键字，所以后面需要加一个下划线）。
beautifulsoup4库还提供了如下其他查询方法。
find_parents()和find_parent()：前者返回所有祖先节点，后者返回直接父节点。
find_next_siblings()和find_next_sibling()：前者返回后面所有的兄弟节点，后者返回后面的第一个兄弟节点。
find_previous_siblings()和find_previous_sibling()：前者返回前面所有的兄弟节点，后者返回前面的第一个兄弟节点。
find_all_previous()和find_previous()：前者返回节点前所有符合条件的节点，后者返回节点前第一个符合条件的节点。
```

### 3.CSS选择器

```
beautifulsoup4库还提供了使用CSS选择器来选择节点的方法（只需要调用select()方法传入相应的CSS选择器即可）
```

**注意**

```
Tag对象的string属性返回标签的文本信息，而text属性返回标签下所有标签的文本信息。
```

## 四.正则式

**注意**

```
search()方法返回Match对象，它的group()方法可以返回Match对象的一个或多个子组，而group(1)为第一个子组值。
```

### 1.基础

```
正则表达式是用于处理字符串的强大工具，它使用预定义的特定模式去匹配一类具有共同特征的字符串，主要用于快速、准确地完成复杂字符串的查找、替换等。
```

| 元字符和语法 | 说 明                        | 表达式实例                            |
| ------------ | ---------------------------- | ------------------------------------- |
| 字 符        |                              |                                       |
| 一般字符     | 匹配自身                     | python匹配python                      |
| .            | 匹配除换行符外的任意单个字符 | a.c匹配abc、acc等                     |
| \            | 转义字符                     | a\.c匹配a.c；a\\c匹配a\c              |
| [ ]          | 表示一组字符                 | a[bcd]e（同a[b-d]e）匹配abe、ace和ade |
| [^     ]     | 不在[ ]中的字符              | [^abc]匹配除a，b，c之外的字符         |

  预定义字符集（可用在字符集[  ]中）  

| \d   | 匹配任意数字，等价于[0-9]          | a\dc匹配a1c、a2c等 |
| ---- | ---------------------------------- | ------------------ |
| \D   | 匹配任意非数字                     | a\Dc匹配abc、asc等 |
| \s   | 匹配任意空白字符，等价于[\t\n\r\f] | a\sc匹配a c        |
| \S   | 匹配任意非空字符                   | a\Sc匹配abc等      |
| \w   | 匹配数字、字母、下划线             | a\wc匹配a1c、abc等 |
| \W   | 匹配非数字、字母、下划线           | a\Wc匹配a c        |

| 元字符和语法                   | 说 明                                                        | 表达式实例            |
| ------------------------------ | ------------------------------------------------------------ | --------------------- |
| 数量词（可用在字符或(  )之后） |                                                              |                       |
| *                              | 匹配位于*之前的字符0次或多次                                 | abc*匹配ab、abccc等   |
| +                              | 匹配位于+之前的字符1次或多次                                 | abc+匹配abc、abccc等  |
| ?                              | 匹配位于?之前的字符0次或1次，当此字符紧随任何其他限定符（*、+、？、{m}、{m,n}）之后时，匹配模式为“非贪婪” | abc?匹配ab和abc       |
| {m}                            | 匹配前一个字符*m*次                                          | ab{2}c匹配abbc        |
| {m,n}                          | 匹配前一个字符*m*至*n*次，省略m则匹配0至*n*次；省略n则匹配*m*至无限次 | ab{1,2}c匹配abc和abbc |

| 边界匹配 |          |                       |
| -------- | -------- | --------------------- |
| ^        | 匹配行首 | ^abc匹配以abc开始的行 |
| $        | 匹配行尾 | abc$匹配以abc结尾的行 |

| 逻辑、分组 |                               |                    |
| ---------- | ----------------------------- | ------------------ |
| \|         | 匹配位于\|之前或之后的字符    | a\|b匹配a或b       |
| ( )        | 将位于( )内的内容作为一个整体 | (abc){2}匹配abcabc |

```
正则表达式通常用于在文本中查找匹配的字符串。Python中数量词默认是贪婪的，即总是尝试匹配尽可能多的字符；相反，非贪婪总是尝试匹配尽可能少的字符。例如，正则表达式“ab*”如果用于查找字符串“abbbc”，将找到“abbb”；而如果使用表达式“ab*?”，将找到“a”。

具体应用时，可以单独使用某种类型的元字符，但处理复杂字符串时，经常需要将多个正则表达式元字符进行组合。下面给出了几个示例。
（1）'[a-zA-Z0-9]'可以匹配一个任意大小写字母或数字。
（2）'^(\w){6,15}$'匹配长度为6～15的字符串，可以包含数字、字母和下划线。
（3）'^\w+@(\w+\.)+\w+$'检查给定字符串是否为合法电子邮件地址。
（4）'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$'检查给定字符串是否为合法IP地址。
```

### 2.re

```
其中，函数参数pattern为正则表达式；参数string为字符串；参数flags的值可以是re.I（忽略大小写）、re.M（多行匹配模式）和re.S（匹配包含换行符在内的所有字符）等。
```

| 方  法                                                       | 描  述                                                       |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| re.compile(pattern[,  flags])                                | 用于编译正则表达式，生成一个正则表达式（Pattern）对象        |
| re.search(pattern,  string[, flags])或  search(string[,  pos[, endpos]]) | 扫描整个字符串并返回第一个成功的匹配                         |
| re.match(pattern,  string[, flags])或  match(string[,  pos[, endpos]]) | 尝试从字符串的起始位置匹配一个模式，返回Match对象或None      |
| re.findall(pattern,  string[, flags])或  findall(string[,  pos[, endpos]]) | 在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表 |
| re.sub(pattern,  repl, string[, count=0])或  sub(repl,  string[, count]) | 用于替换字符串中的匹配项                                     |
| re.split(pattern,  string[, maxsplit=0])或split(string[, maxsplit]) | 按照能够匹配的子串将字符串分割后返回列表                     |

## 五.存储

### 1.JSON

json简单说就是javascript中的对象和数组，所以这两种结构就是对象和数组两种结构，通过这两种结构可以表示各种复杂的结构

```
JavaScript对象标记（Javascript Object Notation，JSON）是一种轻量级的文本数据交换格式。它通过对象和数组的组合来表示数据，构造简洁但是结构化程度非常高。Python提供了json库来实现对JSON文件的读写操作
```

**注意**

JSON数据的书写格式是：键-值对，如{"age"：18}。键是字符串，值可以是对象、数组、数字（整数或浮点数）、布尔值、null和字符串，此处字符串必须要用双引号引起来，不能用单引号。

> 1. 对象：对象在js中表示为`{ }`括起来的内容，数据结构为 `{ key：value, key：value, ... }`的键值对的结构，在面向对象的语言中，key为对象的属性，value为对应的属性值，所以很容易理解，取值方法为 对象.key 获取属性值，这个属性值的类型可以是数字、字符串、数组、对象这几种。
> 2. 数组：数组在js中是中括号`[ ]`括起来的内容，数据结构为 `["Python", "javascript", "C++", ...]`，取值方式和所有语言中一样，使用索引获取，字段值的类型可以是 数字、字符串、数组、对象几种。
> 3. json模块提供了四个功能：`dumps`、`dump`、`loads`、`load`，用于字符串 和 python数据类型间进行转换。

#### a. json.loads()

把Json格式字符串解码转换成Python对象 从json到python的类型转化对照如下：

| JSON         | Python   |
| ------------ | -------- |
| object       | dict     |
| array        | list     |
| string       | unicode  |
| number(int)  | int,long |
| number(real) | float    |
| true         | True     |
| false        | False    |
| null         | None     |

```python
# json_loads.py

import json

strList = '[1, 2, 3, 4]'

strDict = '{"city": "北京", "name": "大猫"}'

json.loads(strList)
# [1, 2, 3, 4]

json.loads(strDict) # json数据自动按Unicode存储
# {u'city': u'\u5317\u4eac', u'name': u'\u5927\u732b'}
```

#### b. json.dumps()

实现python类型转化为json字符串，返回一个str对象 把一个Python对象编码转换成Json字符串

从python原始类型向json类型的转化对照如下：

| JSON         | Python   |
| ------------ | -------- |
| object       | dict     |
| array        | list     |
| string       | unicode  |
| number(int)  | int,long |
| number(real) | float    |
| true         | True     |
| false        | False    |
| null         | None     |

```python
# json_dumps.py

import json
import chardet

listStr = [1, 2, 3, 4]
tupleStr = (1, 2, 3, 4)
dictStr = {"city": "北京", "name": "大猫"}

json.dumps(listStr)
# '[1, 2, 3, 4]'
json.dumps(tupleStr)
# '[1, 2, 3, 4]'

# 注意：json.dumps() 序列化时默认使用的ascii编码
# 添加参数 ensure_ascii=False 禁用ascii编码，按utf-8编码
# chardet.detect()返回字典, 其中confidence是检测精确度

json.dumps(dictStr)
# '{"city": "\\u5317\\u4eac", "name": "\\u5927\\u5218"}'

chardet.detect(json.dumps(dictStr))
# {'confidence': 1.0, 'encoding': 'ascii'}

print json.dumps(dictStr, ensure_ascii=False)
# {"city": "北京", "name": "大刘"}

chardet.detect(json.dumps(dictStr, ensure_ascii=False))
# {'confidence': 0.99, 'encoding': 'utf-8'}
```

***chardet是一个非常优秀的编码识别模块，可通过pip安装\***

#### c. json.dump()

将Python内置类型序列化为json对象后写入文件

```python
# json_dump.py

import json

listStr = [{"city": "北京"}, {"name": "大刘"}]
json.dump(listStr, open("listStr.json","w"), ensure_ascii=False)

dictStr = {"city": "北京", "name": "大刘"}
json.dump(dictStr, open("dictStr.json","w"), ensure_ascii=False)
```

#### d. json.load()

读取文件中json形式的字符串元素 转化成python类型

```python
# json_load.py

import json

strList = json.load(open("listStr.json"))
print strList

# [{u'city': u'\u5317\u4eac'}, {u'name': u'\u5927\u5218'}]

strDict = json.load(open("dictStr.json"))
print strDict
# {u'city': u'\u5317\u4eac', u'name': u'\u5927\u5218'}
```

#### e.写入json

利用dumps()方法可以将Python数据类型转化为JSON格式的字符串，然后调用文件的write()方法写入文本。dumps()方法原型如下：

```
dumps(obj,skipkeys=False,ensure_ascii=True,check_circular=True,allow_nan=True,cls=None,indent=None,separators=None,default=None,sort_keys=False,**kw)

（1）obj：Python数据序列。
（2）skipkeys：表示是否跳过非Python基本类型的键，默认值为False，设置为True时，表示跳过此类键。
（3）ensure_ascii：表示显示格式，默认为True，如果需要输出中文字符，需要将这个参数设置为False，并在写入文件时规定文件输出的编码。
（4）indent：表示输出时缩进字符的个数。
（5）sort_keys：表示是否根据键的值进行排序，默认为False，设置为True时数据将根据键的值进行排序。
```

#### f.读入json

利用loads()方法可以将JSON格式的字符串转化为Python数据类型，如果从JSON文件中读取内容，可以先调用文件的read()方法读取文本内容，然后再进行转换。

### 2.JsonPath（了解）

JsonPath 是一种信息抽取类库，是从JSON文档中抽取指定信息的工具，提供多种语言实现版本，包括：Javascript, Python， PHP 和 Java。

JsonPath 对于 JSON 来说，相当于 XPATH 对于 XML。

> 下载地址：[https://pypi.python.org/pypi/jsonpath](https://pypi.python.org/pypi/jsonpath/)
>
> 安装方法：点击`Download URL`链接下载jsonpath，解压之后执行`python setup.py install`
>
> 官方文档：[http://goessner.net/articles/JsonPath](http://goessner.net/articles/JsonPath/)

###### JsonPath与XPath语法对比：

Json结构清晰，可读性高，复杂度低，非常容易匹配，下表中对应了XPath的用法。

| XPath | JSONPath  | 描述                                                         |
| :---: | --------- | ------------------------------------------------------------ |
|  `/`  | `$`       | 根节点                                                       |
|  `.`  | `@`       | 现行节点                                                     |
|  `/`  | `.`or`[]` | 取子节点                                                     |
| `..`  | n/a       | 取父节点，Jsonpath未支持                                     |
| `//`  | `..`      | 就是不管位置，选择所有符合条件的条件                         |
|  `*`  | `*`       | 匹配所有元素节点                                             |
|  `@`  | n/a       | 根据属性访问，Json不支持，因为Json是个Key-value递归结构，不需要。 |
| `[]`  | `[]`      | 迭代器标示（可以在里边做简单的迭代操作，如数组下标，根据内容选值等） |
|  \|   | `[,]`     | 支持迭代器中做多选。                                         |
| `[]`  | `?()`     | 支持过滤操作.                                                |
|  n/a  | `()`      | 支持表达式计算                                               |
| `()`  | n/a       | 分组，JsonPath不支持                                         |

我们以拉勾网城市JSON文件 http://www.lagou.com/lbs/getAllCitySearchLabels.json 为例，获取所有城市。

```python
# jsonpath_lagou.py

import requests
import jsonpath
import json
import chardet

url = 'http://www.lagou.com/lbs/getAllCitySearchLabels.json'
response = equests.get(url)
html = response.text

# 把json格式字符串转换成python对象
jsonobj = json.loads(html)

# 从根节点开始，匹配name节点
citylist = jsonpath.jsonpath(jsonobj,'$..name')

print citylist
print type(citylist)
fp = open('city.json','w')

content = json.dumps(citylist, ensure_ascii=False)
print content

fp.write(content.encode('utf-8'))
fp.close()
```

### 3.csv文件

```
字符分隔符（Comma-Separated Values，CSV）也称逗号分隔符，其文件以纯文本形式存储表格数据。CSV文件由任意数目的记录组成，记录间以某种换行符分隔；每条记录由字段组成，字段间的分隔符是其他字符或字符串，最常见的是逗号或制表符。通常，所有记录都有完全相同的字段序列，结构简单清晰。Python提供了csv库来实现CSV文件的读写操作。
```

#### 1).写入csv

```
csv库提供了初始化写入对象的writer()方法，还提供了writerow()方法（写入一行）和writerows()方法（写入多行）用于写入文件。
```

**注意**

```
在打开文件写入时，需要设置newline参数为“newline=”，如果不设置，则每写入一行后将会写入一个空行。
```

#### 2).读入csv

```
读取CSV文件时，可通过调用reader()方法返回一个可迭代对象，此对象只能迭代一次，不能直接输出，须调用list()方法将其转换为列表输出。
```

**注意**

```
csv库还提供了DictWriter()方法用于初始化一个字典写入对象，writeheader()方法用于写入表头，DictReader()方法用于将读取的数据转化成字典形式。
```

```
使用Excel打开CSV文件时会出现文字乱码的现象，这是因为在简体中文环境下，Excel打开的CSV文件默认是ANSI编码，如果CSV文件的编码方式为UTF-8、Unicode等，可能会出现文字乱码的情况。解决方法是：用记事本打开CSV文件，选择“文件”→“另存为”选项，打开“另存为”对话框，在“编码”下拉列表中选择“ANSI”选项（见图3-27），单击“保存”按钮将其另存为ANSI编码格式的文件，然后使用Excel打开，即可正常显示文字
```

# 三、爬取动态加载数据

## 一.概念

```
有时用户在使用urllib库或requests库爬取网页时，爬取到的结果可能和在浏览器中看到的不一样，在浏览器中正常显示的页面数据，在爬取到的结果中却没有。这是因为urllib库或requests库发送请求返回的是网页源代码（网页源代码中不包含由JavaScript动态加载的数据），而浏览器中的页面是经过JavaScprit动态加载的。下面以京东搜索“Python”网页为例来说明什么是动态加载数据。
打开Google Chrome浏览器，访问https://search.jd.com/Search?keyword=Python，向下拖动滚动条到页面底部；按“F12”键，打开浏览器的开发者工具窗口，然后选择“Elements”选项，在显示的代码中可以查看到“金融大数据分析 第2版”（该页显示的搜索的最后一本图书）的HTML信息，如图4-1所示。
```

```
动态网页是相对于静态网页而言的，静态网页在不修改页面代码的情况下，其页面的内容和显示效果是基本不变的；而动态网页则不同，页面代码虽然没有变，但是显示的内容却可以随着时间、环境或者数据库操作的结果发生改变。
```

## 二.分析界面

```
JavaScript动态加载数据一般使用Ajax技术（Ajax指异步JavaScript和XML，是一种创建交互式、快速动态网页的网页开发技术），它通过在后台与服务器进行数据交换，实现网页的异步更新。在确认想要获取的网页数据是通过Ajax动态加载的，这时，可通过分析Ajax请求获取真实的URL，然后使用urllib库或requests库构造并发送同样的请求即可。
```

```
在网络（Network）面板中，XHR是Ajax中的概念，表示XMLHttpRequest。在一般情况下，JavaScript加载的文件隐藏在XHR或JS中。
```

```
【问题分析】 
（1）在Google Chrome浏览器中访问https://search.jd.com/Search?
keyword=Python，按“F12”键打开开发者工具窗口，选择“Network”选项。
（2）在打开的网络（Network）面板中选择“XHR”选项，向下拖动网页的滚动条，可以看到新的HTTP请求信息，
（3）单击“s_new.php?keyword=python&wq=python&page=2&s=27&scrolling=y&log_id=1594554463534.3400”请求，然后选择“Preview”选项，即可显示后30本图书的信息
（4）选择
“Headers”选项，查看HTTP请求信息，获取发送请求所需的“Request URL”和“Request Headers”信息（Cookie、Referer和User-Agent）
（5）Request URL中的log_id参数是Unix时间戳，表示当前的时间；末尾的一串数字共30个，表示该网页前30本图书的ID；还有tpl=2_M参数，实际发送请求时都可以去掉。因此，Request URL最终可简述为https://search.jd.com/s_new.php?keyword=python&page=2&s=27&scrolling=y。
```

## 三.selenium

### 1.简介

```
实际动态网页中，很多Ajax请求的参数是加密的，用户很难通过分析Ajax请求获取真实的URL，还有一些动态加载数据并不是Ajax生成的，此时，可以使用Selenium模拟浏览器的方法来获取网页动态加载和渲染的数据。
Selenium是一个自动化测试工具，利用它可以驱动浏览器执行特定的动作（如单击、输入等操作）。同时还可以获取浏览器当前呈现的页面内容，做到可见即可爬。
```

### 2.使用

#### 1).声明浏览器对象

```
selenium库支持多种浏览器。例如，Google Chrome、Firefox、Edge等网页浏览器， Android、BlackBerry等手机端的浏览器以及PhantomJS无界面浏览器等。其初始化方法如下。
from selenium import webdriver      #导入webdriver模块
brower = webdriver.Chrome()	#初始化Google Chrome浏览器对象
brower = webdriver.Firefox()	#初始化Firefox浏览器对象
brower = webdriver.Edge()		#初始化Edge浏览器对象
brower = webdriver.PhantomJS()	#初始化PhantomJS浏览器对象
```

**声明浏览器对象前须安装对应的浏览器和驱动。**

#### 2).访问网页

```
使用get()方法请求网页（传入URL参数即可），会弹出浏览器窗口并自动访问网页，然后可以调用浏览器对象的属性和方法获取网页的信息
```

| 属性和方法           | 说  明                                      |
| -------------------- | ------------------------------------------- |
| page_source          | 获取当前页面HTML源代码                      |
| current_url          | 获取当前页面URL                             |
| title                | 获取当前页面HTML源代码中title标签的文本信息 |
| get_cookies()        | 获取所有cookie                              |
| get_cookie(name)     | 获取指定cookie                              |
| add_cookie({})       | 添加cookie，参数为字典类型                  |
| delete_all_cookies() | 删除所有cookie                              |
| delete_cookie(name)  | 删除指定cookie                              |

#### 3).定位节点

```
selenium库提供了一系列定位节点的方法
```

| 方 法                               | 说 明                            |
| ----------------------------------- | -------------------------------- |
| find_element_by_class_name()        | 通过节点的class属性值定位        |
| find_element_by_name()              | 通过节点的name属性名定位         |
| find_element_by_id()                | 通过节点的id属性值定位           |
| find_element_by_link_text()         | 通过超链接节点的文本定位         |
| find_element_by_partial_link_text() | 通过超链接节点包含的部分文本定位 |
| find_element_by_tag_name()          | 通过节点名定位                   |
| find_element_by_xpath()             | 使用XPath语法定位                |
| find_element_by_css_selector()      | 使用CSS选择器定位                |

以上方法匹配的是第一个符合条件的节点，如果把方法名中的“element”改为“elements”，匹配的将是所有符合条件的节点，返回一个列表。调用方法定位到节点后会返回一个WebElement类型的对象，该对象提供了一些方法来提取所需的信息

| 方 法           | 说 明      |
| --------------- | ---------- |
| get_attribute() | 获取属性   |
| text            | 获取文本   |
| tag_name        | 获取标签名 |
| id              | 获取节点id |

```
selenium库还提供了通用方法find_element()，该方法须传入查找方式和对应的值两个参数。例如，find_element(By.ID,id)等价于find_element_by_id(id)。注意：使用时须导入selenium.webdriver.common.by的By模块。
```

#### 4).模拟浏览器操作

```
selenium库提供了模拟浏览器执行一些操作的方法
使用ActionChains类初始化对象（如ActionChains(browser)），返回一个ActionChains对象。该对象存储所有用户产生的行为，再通过perform()方法执行这些行为。
```

| 对 象  | 方 法             | 说 明                      |
| :----: | ----------------- | -------------------------- |
|  节点  | send_keys(string) | 输入文字，参数类型为字符串 |
|  节点  | clear()           | 清除文字                   |
|  节点  | click()           | 单击节点                   |
| 浏览器 | maximize_window() | 最大化浏览器窗口           |
| 浏览器 | minimize_window() | 最小化浏览器窗口           |
| 浏览器 | forward()         | 页面前进                   |
| 浏览器 | back()            | 页面后退                   |
| 浏览器 | refresh()         | 页面刷新                   |
| 浏览器 | switch_to         | 切换窗口或嵌套页面         |
| 浏览器 | close()/quit()    | 关闭浏览器                 |

**注意**

```
swith_to用于切换不同窗口或嵌套页面。例如，使用switch_to.window()切换窗口，参数是选项卡的代号（调用windows_handles属性获取当前开启的所有选项卡，返回选项卡的代号列表）；当页面存在iframe标签时，使用switch_to.frame()切换到嵌套页面，参数可以直接是标签的id属性或name属性，也可以是定位的节点；使用switch_to.alert()切换到警告框。
```

在实际Web测试中，会有很多鼠标的操作，不单单只有单击，有时候还要用到右击、双击、拖动等操作。selenium库中的ActionChains类提供了实现这些操作的方法

| 方 法                  | 说 明                    |
| ---------------------- | ------------------------ |
| click(element)         | 单击鼠标左键             |
| content_click(element) | 单击鼠标右键             |
| double_click(element)  | 双击鼠标左键             |
| drag_and_drop(element) | 拖动鼠标到某个节点后松开 |

| 方 法                                                | 说 明                                          |
| ---------------------------------------------------- | ---------------------------------------------- |
| move_to_element(element)                             | 鼠标移动到某个节点                             |
| click_and_hold(element)                              | 鼠标左键按住某个节点，不松开                   |
| key_down(value,  element)                            | 按下特殊键，只能用Ctrl、Alt或Shift键，如Ctrl+C |
| key_up(value,  element)                              | 松开特殊键                                     |
| move_by_offset(xoffset,yoffset)                      | 鼠标从当前位置按偏移移动                       |
| move_to_element_with_offset(element,xoffset,yoffset) | 鼠标移动到某个节点并偏移                       |
| perform()                                            | 执行操作产生的行为                             |
| release(element)                                     | 在某个节点位置松开鼠标左键                     |

**注意**

```
Selenium API没有提供某些操作（如下拉滚动条）。在这种情况下，可以直接模拟运行JavaScript，使用execute()方法即可实现。例如，execute_script("window.scrollTo(0,document.body.scrollHeight)")实现了将滚动条下拉到页面最底部。
```

#### 5).页面等待

很多网页采用了Ajax技术，程序无法确定某个节点是否已经完全加载。如果页面实际加载的时间过长，会导致程序使用未加载出来的节点，此时就会抛出NullPointer异常。为了避免这种情况，selenium库提供了显式等待和隐式等待两种等待方式。

###### a.显式等待

（1）selenium库提供了WebDriverWait类实现显式等待，WebDriverWait的语法格式如下：

```
WebDriverWait(driver, timeout, poll_frequency=0.5, ignored_exceptions=None)
```

其中，driver表示浏览器对象；timeout表示等待时间；poll_frequency表示检查频率，程序会每隔这个时间检查一次，如果满足条件则执行下一步，如果不满足则继续等待，缺省时为0.5 s；ignored_exceptions表示超时后的异常信息。

WebDriverWait类须与until()方法或until_not()方法结合使用。例如，WebDriverWait(driver, 10).until(method, message="")，表示在10 s内，每隔0.5 s，调用until里的方法，直到它满足了预期条件，如果时间超过10 s，则抛出异常。

selenium库的expected_conditions类提供了until()方法和until_not()方法中预期条件判断的方法

| 方 法                                 | 说 明                                       |
| ------------------------------------- | ------------------------------------------- |
| title_is()                            | 标题是内容                                  |
| title_contains()                      | 标题包含某内容                              |
| presence_of_element_located()         | 节点加载完成，传入定位元组，如（BY.ID,'p'） |
| presence_of_all_elements_located()    | 所有节点加载完成                            |
| visibility_of_element_located()       | 节点可见，传入定位元组                      |
| visibility_of()                       | 可见，传入节点对象                          |
| text_to_be_present_in_element()       | 某个节点文本包含某文字                      |
| text_to_be_present_in_element_value() | 某个节点值包含某文字                        |

| 方 法                                    | 说 明                                             |
| ---------------------------------------- | ------------------------------------------------- |
| frame_to_be_available_and_switch_to_it() | 加载并切换                                        |
| invisibility_of_element_located()        | 节点不可见                                        |
| element_to_be_clicked()                  | 节点可单击                                        |
| staleness_of()                           | 判断节点是否仍在DOM中，常用于判断页面是否已刷新   |
| element_to_be_selected()                 | 节点可选择，传入节点对象                          |
| element_located_to_be_selected()         | 节点可选择，传入定位元组                          |
| element_selection_state_to_be()          | 传入节点对象和状态，相等则返回True，否则返回False |
| element_located_selection_state_to_be()  | 传入定位元组和状态，相等则返回True，否则返回False |
| alert_is_present()                       | 是否出现警告                                      |

###### b.隐式等待

```
selenium库直接使用implicitly_wait(timeout)方法实现隐式等待。该方法表示在规定的时间内页面的所有元素都加载完了就执行下一步，否则一直等到时间截止，然后再继续下一步。
```

## 四.数据库

### 1.mysql

#### 1).语法

```
pymysql.connect(host, port, user, password, db, charset, connect_timeout, use_unicode)

（1）host：数据库地址，本机地址通常为127.0.0.1，也可设置为localhost，默认为None。
（2）port：数据库端口，通常为3306，默认为0。
（3）user：数据库用户名，管理员用户为root，默认为None。
（4）password：表示数据库密码，默认为None。
（5）db：表示数据库库名，无默认值。
（6）charset：表示插入数据库时的编码，默认为None。
（7）connect_timeout：表示连接超时时间，以秒为单位，默认为10。
（8）use_unicode：表示结果以unicode字符串的格式返回，默认为None。
```

| 方 法      | 说 明                                                        |
| ---------- | ------------------------------------------------------------ |
| cursor()   | 创建一个游标对象，所有SQL语句的执行都需要在游标对象下进行    |
| commit()   | 提交事务，对支持事务的数据库或表，若提交修改操作后，不使用该方法，则不会写入数据库中 |
| rollback() | 事务回滚，在没有commit()的前提下，执行此方法时，回滚当前事务 |
| close()    | 断开连接                                                     |

#### 2).游标

| 方 法            | 说 明                       |
| ---------------- | --------------------------- |
| execute(sql)     | 执行SQL语句                 |
| executemany(sql) | 执行多条SQL语句             |
| fetchone()       | 获取执行结果中的第一条记录  |
| fetchmany(n)     | 获取执行结果中的前*n*条记录 |
| fetchall()       | 获取执行结果的全部记录      |
| scroll()         | 用于游标滚动                |
| close()          | 关闭游标                    |

```
使用connect()方法连接数据库时，可以指定数据库。但是如果还没有创建数据库，需要先进行连接；再使用SQL语句创建数据库，断开连接；然后重新连接指定数据库，选择该数据库进行操作。
插入、更新和删除操作都是对数据库进行更改的操作，使用execute()方法执行完SQL语句后，必须使用commit()方法提交到数据库执行。
```

### 2.mongodb

```
MongoClient类连接数据库，须指定连接的数据库地址（如localhost）和端口（默认为27017），返回MongoClient对象。
MongoClient对象选择数据库，语法格式为“对象.数据库名”（也可写为“对象[‘数据库名’]”），如果数据库不存在则新建一个数据库，返回Database对象。使用Database对象选择集合，语法格式为“对象.集合名”（也可写为“对象[‘集合名’]”），如果集合不存在则新建一个集合，返回Collection对象。
MongoClient对象提供了drop_database()方法删除数据库。
```

Collection对象方法

| 方  法        | 说  明                                                       |
| ------------- | ------------------------------------------------------------ |
| insert_one()  | 插入一条记录，参数为字典键值对                               |
| insert_many() | 插入多条记录，参数为字典列表                                 |
| find_one()    | 查询一条记录，参数为字典键值对，返回一个字典                 |
| find()        | 查询多条记录，参数为字典键值对，返回一个字典列表             |
| count()       | 查询记录计数                                                 |
| update_one()  | 修改第一条匹配的记录，第一个参数为查询的条件，第二个参数为修改的字段 |
| update_many() | 修改所有匹配的记录，第一个参数为查询的条件，第二个参数为修改的字段 |

| 方  法        | 说  明                                                       |
| ------------- | ------------------------------------------------------------ |
| sort()        | 升序或降序排列记录，第一个参数为要排序的字段，第二个参数指定排序规则，1为升序，−1为降序，默认为升序 |
| delete_one()  | 删除第一条匹配的记录，参数为查询的条件                       |
| delete_many() | 删除所有匹配的记录，参数为查询的条件                         |
| drop()        | 删除集合                                                     |

  比较符号 

| 符 号 | 描 述      | 示 例                 | 说 明          |
| ----- | ---------- | --------------------- | -------------- |
| $lt   | 小于       | {'id':{'$lt':4}}      | id值小于4      |
| $gt   | 大于       | {'id':{'$gt':4}}      | id值大于4      |
| $lte  | 小于等于   | {'id':{'$lte':4}}     | id值小于等于4  |
| $gte  | 大于等于   | {'id':{'$gte':4}}     | id值大于等于4  |
| $ne   | 不等于     | {'id':{'$ne':4}}      | id值不等于4    |
| $in   | 在范围内   | {'id':{'$in':[4,8]}}  | id值等于4或8   |
| $nin  | 不在范围内 | {'id':{'$nin':[4,8]}} | id值不等于4和8 |

| 符 号   | 描 述                        | 示 例                         | 说 明                      |
| ------- | ---------------------------- | ----------------------------- | -------------------------- |
| $regex  | 匹配正则表达式               | {'name':{'$regex':'^J.*'}}    | name值以“J”开头            |
| $exists | 属性是否存在                 | {'name':{'$exists':True}}     | name属性存在               |
| $type   | 类型判断                     | {'name':{'$type':'str'}}      | name值的类型为字符串       |
| $text   | 文字查询，属性是否包含字符串 | {'name':{'$text':'Jack'}}     | name属性中包含“Jack”字符串 |
| $where  | 高级条件查询                 | {'$where':'obj.id==obj.name'} | id值等于name值             |

# 四、反爬策略

## 一.反爬策略

```
编写爬虫程序的目的是自动获取网站的一些数据，而反爬虫则是利用技术手段防止爬虫程序爬取数据。反爬虫的原因如下：
    （1）不遵守规范的爬虫会影响网站的正常使用，很多初级爬虫程序非常简单，不考虑网站服务器的压力，有时甚至会导致网站宕机。
    （2）保护数据，不希望重要或涉及用户利益的数据被别人爬取。
    （3）避免数据被同行爬取，丧失竞争力，如电商行业。
    常见的反爬虫策略有以下几种。
```

### 1．通过Headers反爬虫

```
通过识别用户请求的Headers来反爬虫是网站服务器最常用的反爬虫策略。很多网站都会对HTTP请求头部的User-Agent进行检测（判断是否为浏览器访问）；有一部分网站会对Referer进行检测（一些资源网站的防盗链接）；还有一部分网站会对Cookie进行检测（需要登录才能获取更多数据）。
```

### 2．基于用户行为反爬虫

```
通过检测用户行为来判断请求是否来自爬虫程序也是一种常用的反爬虫策略。例如，同一IP地址短时间内多次访问，或者同一账户短时间内多次进行相同操作，都有可能使网站服务器采取反爬虫措施。
```

### 3．采用动态加载数据反爬虫

```
有一些网站的网页是通过JavaScript动态生成的，无法直接爬取当前网页获取所需数据，这样对爬虫程序的直接爬取造成了一些困难。
```

**常见的爬虫和反爬虫对抗过程**

![image-20240709210055210](D:/%E6%96%87%E6%A1%A3/%E7%AC%94%E8%AE%B0/%E6%8A%80%E6%9C%AF/%E7%88%AC%E8%99%AB/image-20240709210055210.png)

## 二.应对策略

### 1.使用代理IP

```
针对网站检测IP访问的反爬虫策略，可以使用代理IP。代理IP是代理用户取得网络信息的IP地址，它可以帮助爬虫程序掩藏真实身份，突破IP访问的限制，隐藏爬虫程序的真实IP，从而避免被网站的反爬虫程序禁止。
requests库实现使用代理IP非常方便，只需要构造一个代理IP的字典（如{'http':'http://121.232.148.167:9000'}），然后在发送HTTP请求时，使用proxies参数添加代理IP的字典即可。如果需要使用多个代理IP，可将所有的代理IP字典构成列表，然后从列表中随机选择代理IP。
```

**注意**

```
有很多网站提供免费的代理IP（如https://www.kuaidaili.com/free/inha），但是网上公布的代理IP不一定可用，用户在使用之前最好验证一下代理IP的可用性。
```

### 2 .降低请求频率

```
爬虫程序运行得太频繁，一方面对网站极不友好，另一方面十分容易触发网站的反爬虫。因此，当运行爬虫程序时，可以设置两次请求之间的时间间隔来降低请求频率（通过time库设置程序休眠时间来实现）。
如果时间间隔设置为一个固定数字，可能使爬虫程序不太像正常用户的行为，所以可通过random库设置随机数来设置随机时间间隔。
```

**注意**

```
   用户实际浏览网页的过程中，会浏览一个网页一定的时间后浏览其他网页，然后过一段时间继续回来浏览。因此，爬虫程序可以在爬取一定的页数后休眠更长的时间（如设置每爬取5次数据休息10 s）。
```

# 五、模拟登陆

### 1.原理

```
Session会话机制的过程为：当用户通过浏览器登录网站时，会在服务器端生成一个Session，同时会为该Session生成唯一的Session ID，然后通过响应将Session ID（响应头中的Set-Cookie信息）返回给浏览器；当再次访问网站时，浏览器会将Session ID放到请求头中一起发送给服务器，服务器从请求中提取出Session ID，并和保存的所有Session ID进行对比，查找相应的Session，从而实现保持登录。
```

requests库中的session对象可以模拟登录并自动更新Cookie，实现保持登录

| 方法/属性 | 说 明                                                        |
| --------- | ------------------------------------------------------------ |
| session() | 创建一个session对象                                          |
| headers   | session对象的请求头                                          |
| cookies   | session对象的Cookie                                          |
| update()  | 设置或修改session对象的属性，如session.cookies.update({‘id’:  ‘sessionID’})，如果cookies属性中的id信息不存在，则设置该信息；如果存在，则修改该信息为“sessionID” |

# 六、处理验证码

## 1 处理图片验证码

```
图片验证码是一种最简单的验证码，一般由4位字母或数字组成，需要完全正确地输入字符才能完成登录。
```

### 1).识别思路

识别图片验证码通常使用图像识别技术。这种技术称为光学字符识别（Optical Character Recognition，OCR），它通过检测暗、亮的模式确定其形状，然后使用字符识别方法将形状翻译成计算机文字。而在识别过程中，一些图片存在多余线条的干扰，需要对图片进行灰度和二值化处理，以提高图片的识别率。图片验证码的识别思路如下。

**pytesseract**

```
（1）获取验证码图片。在网页的HTML源代码中查找验证码节点，获取链接属性，然后发送请求，获取验证码图片
（2）将图像转化为灰度图像，如图6-7所示。
（3）将灰度图像转化为二值图像。指定二值化的阈值，将像素小于阈值的地方设置成黑色，将像素大于阈值的地方设置成白色，去除图片中的线条，如图6-8所示。
（4）使用OCR技术识别图中字母或数字。
```

### 2).识别验证码

image_to_string()方法是pytesseract库提供的识别图片中文本的方法，它将图片上的字母或数字转为字符串。该方法的参数是Image对象，它可以由PIL库（Python的第三方图像处理库）中的Image模块生成。Image模块还提供了图像基本操作的方法

| 方 法            | 说 明                                                        |
| ---------------- | ------------------------------------------------------------ |
| open()           | 打开图片，返回一个Image对象                                  |
| convert(mode)    | 不同模式图像之间的转换，mode为L，表示灰度图像；mode为1，表示二值图像 |
| point(table,'1') | 点方法，将灰度图像转换成二值图像，table为根据指定二值化阈值生成的像素表 |
| show()           | 显示图片                                                     |
| save()           | 保存图片                                                     |

```
【问题分析】
（1）在Google Chrome浏览器中访问http://www.daimg.com/member/login.php，通过查看和分析HTML源代码，可以看到验证码图片的链接包含在id属性值为“vdimgck”的img节点的src属性中，
（2）获取图片后，调用convert('L')方法将图像转换成灰度图像。
（3）设置二值化阈值为127，将图片的像素进行黑白处理生成像素表，然后调用point(table,'1')方法将灰度图像转换成二值图像。
（4）调用image_to_string()方法将图片上的字母或数字转为字符串，并使用正则表达式提取验证码。
```

验证码的原图、灰度图和二值图

![image-20240709212454054](D:/%E6%96%87%E6%A1%A3/%E7%AC%94%E8%AE%B0/%E6%8A%80%E6%9C%AF/%E7%88%AC%E8%99%AB/image-20240709212454054.png)

**注意**

```
OCR识别图片验证码时，有时识别不准确，实际应用中可以进行多次识别，直到识别正确验证成功为止。
```

## 2 .处理点触验证码

```
点触验证码是给出一张复杂的图片，根据图片上的指示进行点击,即可完成验证。
```

### 1).注册超级鹰账号

```
使用浏览器访问http://www.chaojiying.com/user/reg/，注册并登录网站，然后在“用户中心”的“软件ID”中生成一个软件ID以便在接口中调用

超级鹰是一个收费平台，因此需要在“购买题分”中进行充值（如充值1元，可以购买1000题分）。不同类型的验证码费用不同，在“价格体系”中可以查看各种验证码的费用情况及类型
```

### 2).识别思路

```
自动识别点触验证码的解决方法是获取验证码图片，上传给超级鹰网站获取单击的位置坐标，然后依次单击完成验证。点触验证码的识别思路如下。
    （1）获取验证码图片。首先将显示验证码的登录页面全屏截图；然后在网页的HTML源代码中查找验证码图片节点，获取图片的位置和大小；最后根据其左下角和右上角的坐标从页面全屏截图中截取相应的验证码图片并保存。
    （2）获取单击的位置坐标。调用超级鹰接口，上传验证码图片至超级鹰后台，返回识别的坐标。
    （3）解析坐标，模拟点击。由于超级鹰返回的坐标是字符串形式，且每个坐标都以“｜”分割，所以需要对坐标进行解析，然后模拟鼠标的单击操作，依次进行单击。
```

## 3.处理滑动拼图验证码

```
滑动拼图验证码需要拖动滑块拼合图像，若图像完全拼合，则验证成功，否则需要重新验证。
```

### 1).识别思路

```
滑动拼图验证码一般的解决方法是：获取滑块移动至缺口的距离，生成滑动轨迹，然后拖动滑块到缺口处，实现滑动拼图验证。本节使用第三方收费的接口（超级鹰接口）来识别并获取滑块移动至缺口的距离，识别思路如下。
    （1）获取验证码图片。首先，将显示验证码的登录页面全屏截图；然后，在网页的HTML源代码中查找验证码图片节点，获取图片的位置和大小；最后，根据其左下角和右上角的坐标从页面全屏截图中截取相应的验证码图片并保存。	
    （2）在图片上绘制文字。在图片上绘制“请点击缺口左上角”文字并保存，以便超级鹰获取识别坐标的要求
    （3）获取滑块移动至缺口的距离。调用超级鹰接口，上传图片至超级鹰后台，返回识别的坐标，解析x坐标，即缺口左上角到验证码图片左边缘的距离。一般滑块不是从左边缘开始移动，因此需要将x坐标减去滑块左上角到左边缘的距离。
（4）生成滑块移动轨迹。模拟人拖动滑块，前段滑块做匀加速运动，后段滑块做匀减速运动，生成移动轨迹。
（5）模拟拖动滑块。定位底部滑块节点，按照移动轨迹模拟拖动滑块到缺口位置。

```

在图片上绘制文字可以通过PIL库来实现，PIL库中的ImageFont模块提供了设置字体和大小的方法，ImageDraw模块提供了在图片上绘制文字的方法

| 模  块    | 方   法                           | 说   明                                                      |
| --------- | --------------------------------- | ------------------------------------------------------------ |
| ImageFont | truetype(file,size)               | 设置字体和大小，返回一个字体对象，file表示字体文件，size表示字体大小 |
| ImageDraw | Draw(image)                       | 创建一个绘图对象，image表示图片对象                          |
| ImageDraw | text(position,string,  fill,font) | 在图片指定的位置绘制字符串，position为(x,y)形式，表示位置；string表示内容；fill表示颜色；font表示字体对象 |

# 七、HTTP代理神器Fiddler

Fiddler是一款强大Web调试工具，它能记录所有客户端和服务器的HTTP请求。 Fiddler启动的时候，默认IE的代理设为了127.0.0.1:8888，而其他浏览器是需要手动设置。



## 1.工作原理

Fiddler 是以代理web服务器的形式工作的，它使用代理地址：127.0.0.1，端口：8888

![img](D:\文档\笔记\fidder_pro-1710807796530-19.jpg)

## 2.Fiddler抓取HTTPS设置

1. 启动Fiddler，打开菜单栏中的 Tools > Telerik Fiddler Options，打开“Fiddler Options”对话框。

	![img](D:\文档\笔记\01-fidder-1710807807293-22.png)

2. 对Fiddler进行设置：

	- 打开工具栏->Tools->Fiddler Options->HTTPS，

	- 选中Capture HTTPS CONNECTs (捕捉HTTPS连接)，

	- 选中Decrypt HTTPS traffic（解密HTTPS通信）

	- 另外我们要用Fiddler获取本机所有进程的HTTPS请求，所以中间的下拉菜单中选中...from all processes （从所有进程）

	- 选中下方Ignore server certificate errors（忽略服务器证书错误）

		![img](D:\文档\笔记\01-fidder_01-1710807817502-25-1710807819251-27.png)

3. 为 Fiddler 配置Windows信任这个根证书解决安全警告：Trust Root Certificate（受信任的根证书）。

	![img](D:\文档\笔记\01-fidder_03-1710807829444-30.png)

4. Fiddler 主菜单 Tools -> Fiddler Options…-> Connections

	- 选中Allow remote computers to connect（允许远程连接）

	- Act as system proxy on startup（作为系统启动代理）

		![img](D:\文档\笔记\01-fidder_02-1710807838849-33.png)

5. 重启Fiddler，使配置生效（这一步很重要，必须做）。

## 3.Fiddler 如何捕获Chrome的会话

1. 安装SwitchyOmega 代理管理 Chrome 浏览器插件

	![img](D:\文档\笔记\switchyomega-1710807852196-36.png)

2. 如图所示，设置代理服务器为127.0.0.1:8888

	![img](D:\文档\笔记\switchyomega_setting-1710807864327-39.png)

3. 通过浏览器插件切换为设置好的代理。

	![img](D:\文档\笔记\SwitchyOmega_switch-1710807873046-42.png)

## 4.Fiddler界面

设置好后，本机HTTP通信都会经过127.0.0.1:8888代理，也就会被Fiddler拦截到。

![img](D:\文档\笔记\fiddler_show-1710807884111-45.png)

#### 请求 (Request) 部分详解

> 1. Headers —— 显示客户端发送到服务器的 HTTP 请求的 header，显示为一个分级视图，包含了 Web 客户端信息、Cookie、传输状态等。
> 2. Textview —— 显示 POST 请求的 body 部分为文本。
> 3. WebForms —— 显示请求的 GET 参数 和 POST body 内容。
> 4. HexView —— 用十六进制数据显示请求。
> 5. Auth —— 显示响应 header 中的 Proxy-Authorization(代理身份验证) 和 Authorization(授权) 信息.
> 6. Raw —— 将整个请求显示为纯文本。
> 7. JSON - 显示JSON格式文件。
> 8. XML —— 如果请求的 body 是 XML 格式，就是用分级的 XML 树来显示它。

#### 响应 (Response) 部分详解

> 1. Transformer —— 显示响应的编码信息。
> 2. Headers —— 用分级视图显示响应的 header。
> 3. TextView —— 使用文本显示相应的 body。
> 4. ImageVies —— 如果请求是图片资源，显示响应的图片。
> 5. HexView —— 用十六进制数据显示响应。
> 6. WebView —— 响应在 Web 浏览器中的预览效果。
> 7. Auth —— 显示响应 header 中的 Proxy-Authorization(代理身份验证) 和 Authorization(授权) 信息。
> 8. Caching —— 显示此请求的缓存信息。
> 9. Privacy —— 显示此请求的私密 (P3P) 信息。
> 10. Raw —— 将整个响应显示为纯文本。
> 11. JSON - 显示JSON格式文件。
> 12. XML —— 如果响应的 body 是 XML 格式，就是用分级的 XML 树来显示它 。

# 八、正则表达式

实际上爬虫一共就四个主要步骤：

1. 明确目标 (要知道你准备在哪个范围或者网站去搜索)
2. 爬 (将所有的网站的内容全部爬下来)
3. 取 (去掉对我们没用处的数据)
4. 处理数据（按照我们想要的方式存储和使用）

我们在昨天的案例里实际上省略了第3步，也就是"取"的步骤。因为我们down下了的数据是全部的网页，这些数据很庞大并且很混乱，大部分的东西使我们不关心的，因此我们需要将之按我们的需要过滤和匹配出来。

那么对于文本的过滤或者规则的匹配，最强大的就是正则表达式，是Python爬虫世界里必不可少的神兵利器。

> 正则表达式，又称规则表达式，通常被用来检索、替换那些符合某个模式(规则)的文本。
>
> 正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个"规则字符串"，这个"规则字符串"用来表达对字符串的一种过滤逻辑。

给定一个正则表达式和另一个字符串，我们可以达到如下的目的：

> - 给定的字符串是否符合正则表达式的过滤逻辑（"匹配"）；
> - 通过正则表达式，从文本字符串中获取我们想要的特定部分（"过滤"）。

![img](D:\文档\笔记\5.1-1710809187987-53.png)

## 1.正则表达式匹配规则

![img](D:\文档\笔记\5.2-1710809198976-56.png)

## 2.Python 的 re 模块

在 Python 中，我们可以使用内置的 re 模块来使用正则表达式。

有一点需要特别注意的是，正则表达式使用 对特殊字符进行转义，所以如果我们要使用原始字符串，只需加一个 r 前缀，示例：

```python
r'chuanzhiboke\t\.\tpython'
```

## 3.re 模块使用：

1. 使用 `compile()` 函数将正则表达式的字符串形式编译为一个 `Pattern` 对象
2. 通过 `Pattern` 对象提供的一系列方法对文本进行匹配查找，获得匹配结果，一个 Match 对象。
3. 最后使用 `Match` 对象提供的属性和方法获得信息，根据需要进行其他的操作

### 1).compile 函数

compile 函数用于编译正则表达式，生成一个 Pattern 对象，它的一般使用形式如下：

```python
import re

# 将正则表达式编译成 Pattern 对象
pattern = re.compile(r'\d+')
```

在上面，我们已将一个正则表达式编译成 Pattern 对象，接下来，我们就可以利用 pattern 的一系列方法对文本进行匹配查找了。

Pattern 对象的一些常用方法主要有：

> - match 方法：从起始位置开始查找，一次匹配
> - search 方法：从任何位置开始查找，一次匹配
> - findall 方法：全部匹配，返回列表
> - finditer 方法：全部匹配，返回迭代器
> - split 方法：分割字符串，返回列表
> - sub 方法：替换

### 2).match 方法

match 方法用于查找字符串的头部（也可以指定起始位置），它是一次匹配，只要找到了一个匹配的结果就返回，而不是查找所有匹配的结果。它的一般使用形式如下：

```
match(string[, pos[, endpos]])
```

其中，string 是待匹配的字符串，pos 和 endpos 是可选参数，指定字符串的起始和终点位置，默认值分别是 0 和 len (字符串长度)。因此，当你不指定 pos 和 endpos 时，match 方法默认匹配字符串的头部。

当匹配成功时，返回一个 Match 对象，如果没有匹配上，则返回 None。

```python
>>> import re
>>> pattern = re.compile(r'\d+')  # 用于匹配至少一个数字

>>> m = pattern.match('one12twothree34four')  # 查找头部，没有匹配
>>> print (m)
None

>>> m = pattern.match('one12twothree34four', 2, 10) # 从'e'的位置开始匹配，没有匹配
>>> print (m)
None

>>> m = pattern.match('one12twothree34four', 3, 10) # 从'1'的位置开始匹配，正好匹配
>>> print (m)                                         # 返回一个 Match 对象
<_sre.SRE_Match object at 0x10a42aac0>

>>> m.group(0)   # 可省略 0
'12'
>>> m.start(0)   # 可省略 0
3
>>> m.end(0)     # 可省略 0
5
>>> m.span(0)    # 可省略 0
(3, 5)
```

在上面，当匹配成功时返回一个 Match 对象，其中：

- group([group1, ...]) 方法用于获得一个或多个分组匹配的字符串，当要获得整个匹配的子串时，可直接使用 group() 或 group(0)；
- start([group]) 方法用于获取分组匹配的子串在整个字符串中的起始位置（子串第一个字符的索引），参数默认值为 0；
- end([group]) 方法用于获取分组匹配的子串在整个字符串中的结束位置（子串最后一个字符的索引+1），参数默认值为 0；
- span([group]) 方法返回 (start(group), end(group))。

再看看一个例子：

```python
>>> import re
>>> pattern = re.compile(r'([a-z]+) ([a-z]+)', re.I)  # re.I 表示忽略大小写
>>> m = pattern.match('Hello World Wide Web')

>>> print (m)     # 匹配成功，返回一个 Match 对象
<_sre.SRE_Match object at 0x10bea83e8>

>>> m.group(0)  # 返回匹配成功的整个子串
'Hello World'

>>> m.span(0)   # 返回匹配成功的整个子串的索引
(0, 11)

>>> m.group(1)  # 返回第一个分组匹配成功的子串
'Hello'

>>> m.span(1)   # 返回第一个分组匹配成功的子串的索引
(0, 5)

>>> m.group(2)  # 返回第二个分组匹配成功的子串
'World'

>>> m.span(2)   # 返回第二个分组匹配成功的子串
(6, 11)

>>> m.groups()  # 等价于 (m.group(1), m.group(2), ...)
('Hello', 'World')

>>> m.group(3)   # 不存在第三个分组
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
IndexError: no such group
```

### 3).search 方法

search 方法用于查找字符串的任何位置，它也是一次匹配，只要找到了一个匹配的结果就返回，而不是查找所有匹配的结果，它的一般使用形式如下：

```
search(string[, pos[, endpos]])
```

其中，string 是待匹配的字符串，pos 和 endpos 是可选参数，指定字符串的起始和终点位置，默认值分别是 0 和 len (字符串长度)。

当匹配成功时，返回一个 Match 对象，如果没有匹配上，则返回 None。

让我们看看例子：

```python
>>> import re
>>> pattern = re.compile('\d+')
>>> m = pattern.search('one12twothree34four')  # 这里如果使用 match 方法则不匹配
>>> m
<_sre.SRE_Match object at 0x10cc03ac0>
>>> m.group()
'12'
>>> m = pattern.search('one12twothree34four', 10, 30)  # 指定字符串区间
>>> m
<_sre.SRE_Match object at 0x10cc03b28>
>>> m.group()
'34'
>>> m.span()
(13, 15)
```

再来看一个例子：

```python
# -*- coding: utf-8 -*-

import re
# 将正则表达式编译成 Pattern 对象
pattern = re.compile(r'\d+')
# 使用 search() 查找匹配的子串，不存在匹配的子串时将返回 None
# 这里使用 match() 无法成功匹配
m = pattern.search('hello 123456 789')
if m:
    # 使用 Match 获得分组信息
    print ('matching string:',m.group())
    # 起始位置和结束位置
    print ('position:',m.span())
```

执行结果：

```
matching string: 123456
position: (6, 12)
```

### 4).findall 方法

上面的 match 和 search 方法都是一次匹配，只要找到了一个匹配的结果就返回。然而，在大多数时候，我们需要搜索整个字符串，获得所有匹配的结果。

findall 方法的使用形式如下：

```
findall(string[, pos[, endpos]])
```

其中，string 是待匹配的字符串，pos 和 endpos 是可选参数，指定字符串的起始和终点位置，默认值分别是 0 和 len (字符串长度)。

findall 以列表形式返回全部能匹配的子串，如果没有匹配，则返回一个空列表。

看看例子：

```python
import re
pattern = re.compile(r'\d+')   # 查找数字

result1 = pattern.findall('hello 123456 789')
result2 = pattern.findall('one1two2three3four4', 0, 10)

print (result1)
print (result2)
```

执行结果：

```
['123456', '789']
['1', '2']
```

再先看一个栗子：

```python
# re_test.py

import re

#re模块提供一个方法叫compile模块，提供我们输入一个匹配的规则
#然后返回一个pattern实例，我们根据这个规则去匹配字符串
pattern = re.compile(r'\d+\.\d*')

#通过partten.findall()方法就能够全部匹配到我们得到的字符串
result = pattern.findall("123.141593, 'bigcat', 232312, 3.15")

#findall 以 列表形式 返回全部能匹配的子串给result
for item in result:
    print (item)
```

运行结果：

```python
123.141593
3.15
```

### 5).finditer 方法

finditer 方法的行为跟 findall 的行为类似，也是搜索整个字符串，获得所有匹配的结果。但它返回一个顺序访问每一个匹配结果（Match 对象）的迭代器。

看看例子：

```python
# -*- coding: utf-8 -*-

import re
pattern = re.compile(r'\d+')

result_iter1 = pattern.finditer('hello 123456 789')
result_iter2 = pattern.finditer('one1two2three3four4', 0, 10)

print (type(result_iter1))
print (type(result_iter2))

print 'result1...'
for m1 in result_iter1:   # m1 是 Match 对象
    print ('matching string: {}, position: {}'.format(m1.group(), m1.span()))

print 'result2...'
for m2 in result_iter2:
    print ('matching string: {}, position: {}'.format(m2.group(), m2.span()))
```

执行结果：

```
<type 'callable-iterator'>
<type 'callable-iterator'>
result1...
matching string: 123456, position: (6, 12)
matching string: 789, position: (13, 16)
result2...
matching string: 1, position: (3, 4)
matching string: 2, position: (7, 8)
```

### 6).split 方法

split 方法按照能够匹配的子串将字符串分割后返回列表，它的使用形式如下：

```
split(string[, maxsplit])
```

其中，maxsplit 用于指定最大分割次数，不指定将全部分割。

看看例子：

```python
import re
p = re.compile(r'[\s\,\;]+')
print (p.split('a,b;; c   d'))
```

执行结果：

```
['a', 'b', 'c', 'd']
```

### 7).sub 方法

sub 方法用于替换。它的使用形式如下：

```
sub(repl, string[, count])
```

其中，repl 可以是字符串也可以是一个函数：

- 如果 repl 是字符串，则会使用 repl 去替换字符串每一个匹配的子串，并返回替换后的字符串，另外，repl 还可以使用 id 的形式来引用分组，但不能使用编号 0；
- 如果 repl 是函数，这个方法应当只接受一个参数（Match 对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。
- count 用于指定最多替换次数，不指定时全部替换。

看看例子：

```python
import re
p = re.compile(r'(\w+) (\w+)') # \w = [A-Za-z0-9]
s = 'hello 123, hello 456'

print (p.sub(r'hello world', s))  # 使用 'hello world' 替换 'hello 123' 和 'hello 456'
print (p.sub(r'\2 \1', s))        # 引用分组

def func(m):
    print(m)
    return 'hi' + ' ' + m.group(2) #group(0) 表示本身，group(1)表示hello，group(2) 表示后面的数字

print (p.sub(func, s))  #多次sub，每次sub的结果传递给func
print (p.sub(func, s, 1))         # 最多替换一次
```

执行结果：

```
hello world, hello world
123 hello, 456 hello
hi 123, hi 456
hi 123, hello 456
```

## 4.匹配中文

在某些情况下，我们想匹配文本中的汉字，有一点需要注意的是，中文的 unicode 编码范围 主要在 [u4e00-u9fa5]，这里说主要是因为这个范围并不完整，比如没有包括全角（中文）标点，不过，在大部分情况下，应该是够用的。

假设现在想把字符串 title = u'你好，hello，世界' 中的中文提取出来，可以这么做：

```python
import re

title = '你好，hello，世界'
pattern = re.compile(r'[\u4e00-\u9fa5]+')
result = pattern.findall(title)

print (result)
```

注意到，我们在正则表达式前面加上了两个前缀 ur，其中 r 表示使用原始字符串，u 表示是 unicode 字符串。

执行结果:

```
['你好', '世界']
```

## 5.贪婪模式与非贪婪模式

1. 贪婪模式：在整个表达式匹配成功的前提下，尽可能多的匹配 ( * )；
2. 非贪婪模式：在整个表达式匹配成功的前提下，尽可能少的匹配 ( ? )；
3. **Python里数量词默认是贪婪的。**

### 1).示例一 ： 源字符串：`abbbc`

- 使用贪婪的数量词的正则表达式 `ab*` ，匹配结果： abbb。

	> `*` 决定了尽可能多匹配 b，所以a后面所有的 b 都出现了。

- 使用非贪婪的数量词的正则表达式`ab*?`，匹配结果： a。

	> 即使前面有 `*`，但是 `?` 决定了尽可能少匹配 b，所以没有 b。

### 2).示例二 ： 源字符串：`aa<div>test1</div>bb<div>test2</div>cc`

- 使用贪婪的数量词的正则表达式：`<div>.*</div>`
- 匹配结果：`<div>test1</div>bb<div>test2</div>`

> 这里采用的是贪婪模式。在匹配到第一个"`</div>`"时已经可以使整个表达式匹配成功，但是由于采用的是贪婪模式，所以仍然要向右尝试匹配，查看是否还有更长的可以成功匹配的子串。匹配到第二个"`</div>`"后，向右再没有可以成功匹配的子串，匹配结束，匹配结果为"`<div>test1</div>bb<div>test2</div>`"

------

- 使用非贪婪的数量词的正则表达式：`<div>.*?</div>`
- 匹配结果：`<div>test1</div>`

> 正则表达式二采用的是非贪婪模式，在匹配到第一个"`</div>`"时使整个表达式匹配成功，由于采用的是非贪婪模式，所以结束匹配，不再向右尝试，匹配结果为"`<div>test1</div>`"。

[正则表达式测试网址](http://tool.oschina.net/regex/)

## 6.案例：使用正则表达式的爬虫

现在拥有了正则表达式这把神兵利器，我们就可以进行对爬取到的全部网页源代码进行筛选了。

下面我们一起尝试一下爬取内涵段子网站： http://www.neihan8.com/article/list_5_1.html

打开之后，不难看到里面一个一个灰常有内涵的段子，当你进行翻页的时候，注意url地址的变化：

- 第一页url: http: //www.neihan8.com/article/list_5_1 .html
- 第二页url: http: //www.neihan8.com/article/list_5_2 .html
- 第三页url: http: //www.neihan8.com/article/list_5_3 .html
- 第四页url: http: //www.neihan8.com/article/list_5_4 .html

这样我们的url规律找到了，要想爬取所有的段子，只需要修改一个参数即可。 下面我们就开始一步一步将所有的段子爬取下来吧。

### 第一步：获取数据

按照我们之前的用法，我们需要写一个加载页面的方法。

```python
这里我们统一定义一个类，将url请求作为一个成员方法处理。
我们创建一个文件，叫duanzi_spider.py
然后定义一个Spider类，并且添加一个加载页面的成员方法

class Duanzi_spider():
    def __init__(self):
        self.url = "http://www.neihan8.com/article/list_5_%s.html"
        self.headers = {
            "User_Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleW\
            ebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36",
            "Accept-Encoding":None,
            "Accept-Language": "zh-CN,zh;q=0.8"

        }

    def load_page(self,url):
            '''可以复用的页面请求方法
            '''
            response = requests.get(url,timeout=10,headers=self.headers)
            if response.status_code==200:
                    print(response.request.headers)
                    return response.content.decode("gbk")
            else:
                    raise ValueError("status_code is:",response.status_code)
```

- 程序正常执行的话，我们会在屏幕上打印了内涵段子第一页的全部html代码。 但是我们发现，html中的中文部分显示的可能是乱码 。

> 注意 ：对于每个网站对中文的编码各自不同，所以html.decode('gbk')的写法并不是通用写法，根据网站的编码而异

### 第二步：筛选数据

> 接下来我们已经得到了整个页面的数据。 但是，很多内容我们并不关心，所以下一步我们需要进行筛选。 如何筛选，就用到了上一节讲述的正则表达式。

- 首先

```python
import re
```

- 然后, 在我们得到的response中进行筛选匹配。

我们需要一个匹配规则:

> 我们可以打开内涵段子的网页，鼠标点击右键 " 查看源代码 " 你会发现每条段子的内容大致如下

```html
<a href="/article/44959.html"><b>回家奔丧</b></a></h4>
                                                                <div class="f18 mb20">
                                                                                　　一老太太跋山涉水来到部队，看望她的孙子，<br />
　　警卫问：“她找谁？”老太说：“找xx，”警卫打完电话说：<br />
　　“xx三天前说她他奶奶过世，回家奔丧去了，奔丧去了，去了。。”

                                                                </div>
def get_content(self,html):
        '''  根据网页内容，同时匹配标题和段子内容
        '''
        pattern = re.compile(r'<a\shref="/article/\d+\.html">(.*?)</a>.*?<div\sclass="f18 mb20">(.*?)</div>', re.S)
        t = pattern.findall(html)
        result = []
        for i in t:
            temp = []
            for j in i:
                    j = re.sub(r"[<b>|</b>|<br />|<br>|<p>|</p>|\\u3000|\\r\\n|\s]","",j)
                    j = j.replace("&ldqo;",'"').replace("&helli;","...").replace("&dqo;",'"').strip()
                    # j = re.sub(r"[&ldqo;|&dqo;]","\"",j)?
                    # j = re.sub(r"…","...",j)
                    temp.append(j)
                print(temp)
            result.append(temp)
    return result
```

> - 这里需要注意一个是`re.S`是正则表达式中匹配的一个参数。
> - 如果 没有re.S 则是 只匹配一行 有没有符合规则的字符串，如果没有则下一行重新匹配。
> - 如果 加上re.S 则是将 所有的字符串 将一个整体进行匹配，findall 将所有匹配到的结果封装到一个list中。

- ok程序写到这，我们再一次执行一下。

```sh
Power@PowerMac ~$ python duanzi_spider.py
```

### 我们第一页的全部段子，不包含其他信息全部的打印了出来。

- 你会发现段子中有很多 `<p>` , `</p>` 很是不舒服，实际上这个是html的一种段落的标签。
- 在浏览器上看不出来，但是如果按照文本打印会有`<p>`出现，那么我们只需要把我们不希望的内容去掉即可了。
- 我们可以如下简单修改一下 get_content().

```python
j = re.sub(r"[<b>|</b>|<br />|<br>|<p>|</p>|\\u3000|\\r\\n|\s]","",j)
j = j.replace("&ldqo;",'"').replace("&helli;","...").replace("&dqo;",'"').strip()
```

------

### 第三步：保存数据

- 我们可以将所有的段子存放在文件中。比如，我们可以将得到的每个item不是打印出来，而是存放在一个叫 duanzi.txt 的文件中也可以。

```python
def save_content(self,content):
    myFile = open("./duanzi.txt", 'a')
    for temp in content:
        myFile.write("\n"+temp[0]+"\n"+temp[1]+"\n")
        myFile.write("-----------------------------------------------------")
    myFile.close()
```

- 然后我们实现保存的方法 ，当前页面的所有段子就存在了本地的duanzi.txt文件中。

### 第四步：实现循环抓取

- 接下来我们就通过参数的传递对page进行叠加来遍历 内涵段子吧的全部段子内容。
- 同时也通过这个run方法实现整个程序的主要逻辑

```python
def run(self):
        i = 1
        while True:
                html = self.load_page(self.url%i)
                result = self.get_content(html)
                print ("按回车继续...")
                print ("输入 quit 退出")
                command = input()
                if (command == "quit"):
                        break
                i+=1
```

最后，我们执行我们的代码，完成后查看当前路径下的duanzi.txt文件，里面已经有了我们要的内涵段子。

**以上便是一个非常精简使用的小爬虫程序，使用起来很是方便，如果想要爬取其他网站的信息，只需要修改其中某些参数和一些细节就行了。**
